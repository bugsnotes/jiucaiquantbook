# ç¬¬åå…«ç« ï¼šå› å­æŒ–æ˜ä¸ç ”ç©¶

ğŸ¯ **é€‚åˆäººç¾¤**ï¼šæœ‰ç¼–ç¨‹èƒ½åŠ›çš„ä¸“ä¸šç”¨æˆ· | â±ï¸ **å­¦ä¹ å‘¨æœŸ**ï¼š3-4å‘¨

## æœ¬ç« å¯¼è¯»

å› å­ç ”ç©¶æ˜¯é‡åŒ–æŠ•èµ„çš„æ ¸å¿ƒï¼Œä¹Ÿæ˜¯åŒºåˆ†æ™®é€šé‡åŒ–ç­–ç•¥å’Œä¸“ä¸šé‡åŒ–ç­–ç•¥çš„å…³é”®ã€‚åœ¨å¤šå› å­é€‰è‚¡ç­–ç•¥ä¸­ï¼Œå› å­çš„è´¨é‡ç›´æ¥å†³å®šäº†ç­–ç•¥çš„è¡¨ç°ã€‚

æœ¬ç« å°†æ·±å…¥ä»‹ç»å› å­æŠ•èµ„çš„ç†è®ºåŸºç¡€ã€å› å­æ„å»ºæ–¹æ³•ã€æœ‰æ•ˆæ€§æ£€éªŒå’Œå¤šå› å­æ¨¡å‹æ„å»ºçš„å®Œæ•´æµç¨‹ã€‚é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ å°†æŒæ¡å¦‚ä½•åƒä¸“ä¸šé‡åŒ–å›¢é˜Ÿä¸€æ ·è¿›è¡Œç³»ç»ŸåŒ–çš„å› å­ç ”ç©¶ã€‚

**æœ¬ç« å†…å®¹**ï¼š
- ğŸ“š 18.1 å› å­æŠ•èµ„åŸºç¡€ï¼ˆç†è®ºåŸºç¡€å’Œåˆ†ç±»ä½“ç³»ï¼‰
- ğŸ”¨ 18.2 å› å­æ„å»ºæ–¹æ³•ï¼ˆæŠ€æœ¯ã€åŸºæœ¬é¢ã€å¦ç±»å› å­ï¼‰
- âœ… 18.3 å› å­æœ‰æ•ˆæ€§æ£€éªŒï¼ˆICåˆ†æã€åˆ†ç»„å›æµ‹ã€æ­£äº¤åŒ–ï¼‰
- ğŸ¯ 18.4 å¤šå› å­æ¨¡å‹æ„å»ºï¼ˆåˆæˆæ–¹æ³•ã€æƒé‡ä¼˜åŒ–ã€é£é™©æ¨¡å‹ï¼‰

---

## 18.1 å› å­æŠ•èµ„åŸºç¡€

### â— 18.1.1 ä»€ä¹ˆæ˜¯å› å­

**å› å­çš„å®šä¹‰**ï¼š

å› å­ï¼ˆFactorï¼‰æ˜¯æŒ‡èƒ½å¤Ÿè§£é‡Šè‚¡ç¥¨æ”¶ç›Šç‡å·®å¼‚çš„æŸä¸ªå¯é‡åŒ–çš„ç‰¹å¾æˆ–æŒ‡æ ‡ã€‚ç®€å•æ¥è¯´ï¼Œå¦‚æœæŸä¸ªæŒ‡æ ‡èƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬åŒºåˆ†"å¥½è‚¡ç¥¨"å’Œ"åè‚¡ç¥¨"ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯ä¸€ä¸ªæœ‰æ•ˆå› å­ã€‚

**ä¾‹å­**ï¼š

```python
# ç¤ºä¾‹ï¼šå¸‚ç›ˆç‡å› å­
# å‡è®¾ï¼šä½å¸‚ç›ˆç‡çš„è‚¡ç¥¨æœªæ¥æ”¶ç›Šæ›´é«˜ï¼ˆä»·å€¼æŠ•èµ„é€»è¾‘ï¼‰

import pandas as pd
import numpy as np

# å‡è®¾æˆ‘ä»¬æœ‰100åªè‚¡ç¥¨çš„æ•°æ®
data = pd.DataFrame({
    'stock_code': [f'è‚¡ç¥¨{i}' for i in range(100)],
    'pe_ratio': np.random.uniform(5, 50, 100),  # å¸‚ç›ˆç‡ï¼š5-50å€
    'future_return': np.random.randn(100) * 0.1  # æœªæ¥æ”¶ç›Šç‡
})

# æ·»åŠ PEå› å­çš„è´Ÿå€¼ï¼ˆPEè¶Šä½ï¼Œå› å­å€¼è¶Šé«˜ï¼‰
data['pe_factor'] = -data['pe_ratio']

# æŒ‰å› å­å€¼åˆ†æˆ5ç»„
data['group'] = pd.qcut(data['pe_factor'], 5, labels=['G1', 'G2', 'G3', 'G4', 'G5'])

# è§‚å¯Ÿå„ç»„çš„å¹³å‡æ”¶ç›Š
group_returns = data.groupby('group')['future_return'].mean()
print("å„ç»„å¹³å‡æœªæ¥æ”¶ç›Šï¼š")
print(group_returns)

# å¦‚æœG5ï¼ˆä½PEç»„ï¼‰çš„æ”¶ç›Šæ˜¾è‘—é«˜äºG1ï¼ˆé«˜PEç»„ï¼‰ï¼Œ
# è¯´æ˜PEæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ä»·å€¼å› å­
```

**å› å­çš„æœ¬è´¨**ï¼š

1. **ä¿¡æ¯æå–å™¨**ï¼šä»æµ·é‡æ•°æ®ä¸­æå–å¯¹æœªæ¥æ”¶ç›Šæœ‰é¢„æµ‹èƒ½åŠ›çš„ä¿¡æ¯
2. **é€‰è‚¡æ ‡å‡†**ï¼šå¸®åŠ©æˆ‘ä»¬ä»ä¼—å¤šè‚¡ç¥¨ä¸­ç­›é€‰å‡ºä¼˜è´¨æ ‡çš„
3. **é£é™©æ”¶ç›Šæ¥æº**ï¼šä¸åŒå› å­ä»£è¡¨äº†ä¸åŒçš„é£é™©æš´éœ²å’Œæ”¶ç›Šæ¥æº

### â— 18.1.2 å› å­çš„åˆ†ç±»

**æŒ‰ç…§æ¥æºåˆ†ç±»**ï¼š

```
å› å­åˆ†ç±»
â”‚
â”œâ”€â”€ æŠ€æœ¯å› å­ï¼ˆTechnical Factorsï¼‰
â”‚   â”œâ”€â”€ åŠ¨é‡ç±»ï¼šè¿‡å»æ”¶ç›Šã€ç›¸å¯¹å¼ºå¼±
â”‚   â”œâ”€â”€ æ³¢åŠ¨ç‡ç±»ï¼šå†å²æ³¢åŠ¨ã€è´å¡”å€¼
â”‚   â”œâ”€â”€ é‡ä»·ç±»ï¼šæˆäº¤é‡ã€æ¢æ‰‹ç‡ã€èµ„é‡‘æµå‘
â”‚   â””â”€â”€ å½¢æ€ç±»ï¼šçªç ´ã€èƒŒç¦»ã€Kçº¿å½¢æ€
â”‚
â”œâ”€â”€ åŸºæœ¬é¢å› å­ï¼ˆFundamental Factorsï¼‰
â”‚   â”œâ”€â”€ ä¼°å€¼ç±»ï¼šPEã€PBã€PSã€PCF
â”‚   â”œâ”€â”€ æˆé•¿ç±»ï¼šè¥æ”¶å¢é•¿ã€åˆ©æ¶¦å¢é•¿ã€ROEå˜åŒ–
â”‚   â”œâ”€â”€ è´¨é‡ç±»ï¼šROEã€æ¯›åˆ©ç‡ã€èµ„äº§å‘¨è½¬ç‡
â”‚   â”œâ”€â”€ ç›ˆåˆ©ç±»ï¼šROAã€å‡€åˆ©ç‡ã€ROIC
â”‚   â””â”€â”€ è´¢åŠ¡å¥åº·ç±»ï¼šè´Ÿå€ºç‡ã€æµåŠ¨æ¯”ç‡ã€ç°é‡‘æµ
â”‚
â”œâ”€â”€ å¦ç±»å› å­ï¼ˆAlternative Factorsï¼‰
â”‚   â”œâ”€â”€ èˆ†æƒ…å› å­ï¼šæ–°é—»æƒ…æ„Ÿã€ç¤¾äº¤åª’ä½“çƒ­åº¦
â”‚   â”œâ”€â”€ åˆ†æå¸ˆå› å­ï¼šè¯„çº§å˜åŒ–ã€ç›ˆåˆ©é¢„æµ‹è°ƒæ•´
â”‚   â”œâ”€â”€ è‚¡ä¸œå› å­ï¼šé«˜ç®¡å¢æŒã€æœºæ„æŒä»“å˜åŒ–
â”‚   â””â”€â”€ äº‹ä»¶å› å­ï¼šå¹¶è´­é‡ç»„ã€è‚¡æƒæ¿€åŠ±
â”‚
â””â”€â”€ é£æ ¼å› å­ï¼ˆStyle Factorsï¼‰
    â”œâ”€â”€ å¸‚å€¼ï¼ˆSizeï¼‰
    â”œâ”€â”€ ä»·å€¼ï¼ˆValueï¼‰
    â”œâ”€â”€ æˆé•¿ï¼ˆGrowthï¼‰
    â”œâ”€â”€ è´¨é‡ï¼ˆQualityï¼‰
    â”œâ”€â”€ åŠ¨é‡ï¼ˆMomentumï¼‰
    â””â”€â”€ æ³¢åŠ¨ç‡ï¼ˆVolatilityï¼‰
```

**ç»å…¸çš„Fama-Frenchå› å­**ï¼š

```python
"""
Fama-Frenchä¸‰å› å­æ¨¡å‹ï¼ˆ1993ï¼‰

R_i - R_f = Î± + Î²_mkt(R_m - R_f) + Î²_smb SMB + Î²_hml HML + Îµ

å…¶ä¸­ï¼š
- R_i - R_f: è‚¡ç¥¨è¶…é¢æ”¶ç›Š
- R_m - R_f: å¸‚åœºè¶…é¢æ”¶ç›Šï¼ˆå¸‚åœºå› å­ï¼‰
- SMB (Small Minus Big): å°å¸‚å€¼è‚¡ç¥¨ - å¤§å¸‚å€¼è‚¡ç¥¨ï¼ˆè§„æ¨¡å› å­ï¼‰
- HML (High Minus Low): é«˜è´¦é¢å¸‚å€¼æ¯” - ä½è´¦é¢å¸‚å€¼æ¯”ï¼ˆä»·å€¼å› å­ï¼‰
"""

class FamaFrenchFactors:
    """Fama-Frenchå› å­æ„å»º"""

    def __init__(self, stock_data):
        """
        å‚æ•°:
            stock_data: DataFrame, åŒ…å«è‚¡ç¥¨çš„å¸‚å€¼ã€è´¦é¢ä»·å€¼ã€æ”¶ç›Šç‡ç­‰
        """
        self.data = stock_data

    def calculate_market_factor(self):
        """å¸‚åœºå› å­ï¼šå¸‚åœºç»„åˆæ”¶ç›Šç‡"""
        # æŒ‰å¸‚å€¼åŠ æƒçš„å¸‚åœºæ”¶ç›Šç‡
        self.data['weight'] = self.data['market_cap'] / self.data['market_cap'].sum()
        market_return = (self.data['return'] * self.data['weight']).sum()
        return market_return

    def calculate_smb_factor(self):
        """SMBå› å­ï¼šå°å¸‚å€¼ - å¤§å¸‚å€¼"""
        # æŒ‰å¸‚å€¼æ’åºï¼Œåˆ†æˆä¸¤ç»„
        median_cap = self.data['market_cap'].median()

        small_cap_return = self.data[
            self.data['market_cap'] <= median_cap
        ]['return'].mean()

        big_cap_return = self.data[
            self.data['market_cap'] > median_cap
        ]['return'].mean()

        smb = small_cap_return - big_cap_return
        return smb

    def calculate_hml_factor(self):
        """HMLå› å­ï¼šé«˜è´¦é¢å¸‚å€¼æ¯” - ä½è´¦é¢å¸‚å€¼æ¯”"""
        # è®¡ç®—è´¦é¢å¸‚å€¼æ¯”ï¼ˆBook-to-Marketï¼‰
        self.data['bm_ratio'] = self.data['book_value'] / self.data['market_cap']

        # åˆ†æˆä¸‰ç»„ï¼šé«˜ã€ä¸­ã€ä½
        quantiles = self.data['bm_ratio'].quantile([0.3, 0.7])

        high_bm_return = self.data[
            self.data['bm_ratio'] >= quantiles[0.7]
        ]['return'].mean()

        low_bm_return = self.data[
            self.data['bm_ratio'] <= quantiles[0.3]
        ]['return'].mean()

        hml = high_bm_return - low_bm_return
        return hml

    def get_all_factors(self):
        """è·å–æ‰€æœ‰å› å­"""
        return {
            'MKT': self.calculate_market_factor(),
            'SMB': self.calculate_smb_factor(),
            'HML': self.calculate_hml_factor()
        }

# ä½¿ç”¨ç¤ºä¾‹
# factors = FamaFrenchFactors(stock_data)
# ff_factors = factors.get_all_factors()
# print(f"å¸‚åœºå› å­: {ff_factors['MKT']:.4f}")
# print(f"SMBå› å­: {ff_factors['SMB']:.4f}")
# print(f"HMLå› å­: {ff_factors['HML']:.4f}")
```

### â— 18.1.3 å› å­æŠ•èµ„ç†è®ºåŸºç¡€

**1. é£é™©è¡¥å¿ç†è®º**

æŸäº›å› å­ä¹‹æ‰€ä»¥æœ‰æ•ˆï¼Œæ˜¯å› ä¸ºå®ƒä»¬ä»£è¡¨äº†æŸç§ç³»ç»Ÿæ€§é£é™©ï¼ŒæŠ•èµ„è€…æ‰¿æ‹…è¿™ç§é£é™©åº”è¯¥è·å¾—è¡¥å¿ã€‚

```
ä¾‹å­ï¼šä»·å€¼å› å­ï¼ˆValue Factorï¼‰

- é«˜è´¦é¢å¸‚å€¼æ¯”çš„è‚¡ç¥¨ï¼ˆä»·å€¼è‚¡ï¼‰é€šå¸¸æ˜¯ç»è¥å›°éš¾çš„å…¬å¸
- æŠ•èµ„è€…æ‰¿æ‹…äº†è¿™äº›å…¬å¸å¯èƒ½ç»§ç»­æ¶åŒ–çš„é£é™©
- å› æ­¤ï¼Œä»·å€¼è‚¡çš„é¢„æœŸæ”¶ç›Šæ›´é«˜ï¼ˆé£é™©è¡¥å¿ï¼‰
- è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆä»·å€¼å› å­é•¿æœŸæœ‰æ•ˆ
```

**2. è¡Œä¸ºé‡‘èå­¦ç†è®º**

æŸäº›å› å­åˆ©ç”¨äº†æŠ•èµ„è€…çš„è¡Œä¸ºåå·®å’Œè®¤çŸ¥é”™è¯¯ã€‚

```python
"""
ç¤ºä¾‹ï¼šåŠ¨é‡å› å­çš„è¡Œä¸ºé‡‘èå­¦è§£é‡Š

åŠ¨é‡æ•ˆåº”ï¼šè¿‡å»è¡¨ç°å¥½çš„è‚¡ç¥¨ç»§ç»­è¡¨ç°å¥½ï¼ˆæ­£åé¦ˆï¼‰
"""

class MomentumBehavioralExplanation:
    """
    åŠ¨é‡å› å­çš„è¡Œä¸ºé‡‘èå­¦è§£é‡Šï¼š

    1. é”šå®šæ•ˆåº”ï¼šæŠ•èµ„è€…è¿‡åº¦å…³æ³¨å†å²ä»·æ ¼ï¼Œè°ƒæ•´ç¼“æ…¢
    2. ç¡®è®¤åå·®ï¼šæŠ•èµ„è€…å€¾å‘äºå¯»æ‰¾æ”¯æŒç°æœ‰è¶‹åŠ¿çš„ä¿¡æ¯
    3. ç¾Šç¾¤æ•ˆåº”ï¼šçœ‹åˆ°ä¸Šæ¶¨åè·Ÿé£ä¹°å…¥ï¼Œæ¨åŠ¨ä»·æ ¼ç»§ç»­ä¸Šæ¶¨
    4. å¤„ç½®æ•ˆåº”ï¼šè¿‡æ—©å–å‡ºç›ˆåˆ©è‚¡ç¥¨ï¼ŒæŒæœ‰äºæŸè‚¡ç¥¨

    è¿™äº›è¡Œä¸ºå¯¼è‡´ä»·æ ¼å¯¹ä¿¡æ¯çš„ååº”ä¸è¶³ï¼ˆUnderreactionï¼‰ï¼Œ
    å½¢æˆäº†çŸ­æœŸåŠ¨é‡æ•ˆåº”ã€‚
    """

    def simulate_momentum_with_underreaction(self, n_periods=100):
        """
        æ¨¡æ‹Ÿä¿¡æ¯ååº”ä¸è¶³å¯¼è‡´çš„åŠ¨é‡æ•ˆåº”
        """
        import matplotlib.pyplot as plt

        # ç”Ÿæˆä¸€ä¸ªæ­£é¢æ¶ˆæ¯
        true_value_change = 10  # çœŸå®ä»·å€¼å˜åŒ–10%

        # æŠ•èµ„è€…é€æ­¥è°ƒæ•´ï¼ˆååº”ä¸è¶³ï¼‰
        price_changes = []
        cumulative_change = 0

        for t in range(n_periods):
            # æ¯æœŸåªååº”ä¸€å°éƒ¨åˆ†
            adjustment = true_value_change * 0.15  # æ¯æ¬¡åªååº”15%
            cumulative_change += adjustment
            price_changes.append(cumulative_change)

            # å½“ç´¯è®¡å˜åŒ–æ¥è¿‘çœŸå®å€¼æ—¶åœæ­¢
            if cumulative_change >= true_value_change * 0.95:
                break

        # å¯è§†åŒ–
        plt.figure(figsize=(10, 6))
        plt.plot(price_changes, label='å®é™…ä»·æ ¼å˜åŒ–ï¼ˆååº”ä¸è¶³ï¼‰', linewidth=2)
        plt.axhline(y=true_value_change, color='r', linestyle='--',
                   label='çœŸå®ä»·å€¼å˜åŒ–', linewidth=2)
        plt.xlabel('æ—¶é—´ï¼ˆå¤©ï¼‰')
        plt.ylabel('ä»·æ ¼å˜åŒ– (%)')
        plt.title('ä¿¡æ¯ååº”ä¸è¶³å¯¼è‡´çš„åŠ¨é‡æ•ˆåº”')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.savefig('momentum_underreaction.png', dpi=300, bbox_inches='tight')
        plt.show()

        return price_changes

# è¿™ç§ååº”ä¸è¶³åˆ›é€ äº†åŠ¨é‡äº¤æ˜“æœºä¼š
```

**3. æ•°æ®æŒ–æ˜ç†è®º**

é€šè¿‡å¤§æ•°æ®åˆ†æå‘ç°çš„ç»Ÿè®¡è§„å¾‹ï¼Œå¯èƒ½æ²¡æœ‰ç»æµå­¦è§£é‡Šï¼Œä½†åœ¨æ ·æœ¬å†…æœ‰æ•ˆã€‚

âš ï¸ **æ³¨æ„**ï¼šçº¯ç²¹çš„æ•°æ®æŒ–æ˜å®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå‘ç°çš„å› å­å¯èƒ½æ˜¯ä¼ªå› å­ï¼

**å› å­æœ‰æ•ˆæ€§çš„ä¸‰ä¸ªå±‚æ¬¡**ï¼š

| å±‚æ¬¡ | è¯´æ˜ | å¯é æ€§ |
|------|------|--------|
| **ç†è®ºæ”¯æ’‘** | æœ‰ç»æµå­¦æˆ–è¡Œä¸ºé‡‘èå­¦ç†è®ºæ”¯æŒ | â­â­â­â­â­ |
| **å®è¯éªŒè¯** | æœ‰é•¿æœŸã€è·¨å¸‚åœºçš„å®è¯è¯æ® | â­â­â­â­ |
| **ç»Ÿè®¡æ˜¾è‘—** | ä»…åœ¨æ ·æœ¬å†…ç»Ÿè®¡æ˜¾è‘— | â­â­ |

---

## 18.2 å› å­æ„å»ºæ–¹æ³•

### â— 18.2.1 æŠ€æœ¯å› å­

æŠ€æœ¯å› å­åŸºäºä»·æ ¼ã€æˆäº¤é‡ç­‰äº¤æ˜“æ•°æ®æ„å»ºï¼Œä¸ä¾èµ–åŸºæœ¬é¢ä¿¡æ¯ã€‚

**1. åŠ¨é‡å› å­**

```python
import pandas as pd
import numpy as np

class MomentumFactors:
    """åŠ¨é‡ç±»å› å­"""

    def __init__(self, price_data):
        """
        å‚æ•°:
            price_data: DataFrame, indexä¸ºæ—¥æœŸï¼Œcolumnsä¸ºè‚¡ç¥¨ä»£ç ï¼Œvaluesä¸ºæ”¶ç›˜ä»·
        """
        self.price = price_data
        self.returns = price_data.pct_change()

    def momentum_simple(self, lookback=20):
        """
        ç®€å•åŠ¨é‡ï¼šè¿‡å»Nå¤©çš„ç´¯è®¡æ”¶ç›Šç‡

        è®¡ç®—å…¬å¼ï¼š(P_t / P_{t-N}) - 1
        """
        momentum = self.price / self.price.shift(lookback) - 1
        return momentum

    def momentum_skip_recent(self, lookback=252, skip=20):
        """
        è·³è¿‡æœ€è¿‘æœŸçš„åŠ¨é‡ï¼ˆç»å…¸åŠ¨é‡å› å­ï¼‰

        è·³è¿‡æœ€è¿‘ä¸€æ®µæ—¶é—´é¿å…çŸ­æœŸåè½¬æ•ˆåº”çš„å¹²æ‰°
        """
        momentum = self.price.shift(skip) / self.price.shift(lookback) - 1
        return momentum

    def momentum_residual(self, lookback=252, skip=20):
        """
        æ®‹å·®åŠ¨é‡ï¼šå»é™¤å¸‚åœºå½±å“åçš„ä¸ªè‚¡åŠ¨é‡

        æ­¥éª¤ï¼š
        1. è®¡ç®—ä¸ªè‚¡æ”¶ç›Šç‡
        2. å¯¹å¸‚åœºæ”¶ç›Šç‡å›å½’ï¼Œå¾—åˆ°æ®‹å·®
        3. ç´¯è®¡æ®‹å·®ä½œä¸ºæ®‹å·®åŠ¨é‡
        """
        # è®¡ç®—ä¸ªè‚¡æ”¶ç›Šç‡
        returns = self.price.pct_change()

        # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯å¸‚åœºæŒ‡æ•°ï¼ˆæˆ–è®¡ç®—å¸‚åœºç­‰æƒæ”¶ç›Šï¼‰
        market_return = returns.mean(axis=1)

        # å¯¹æ¯åªè‚¡ç¥¨ï¼Œè®¡ç®—æ®‹å·®
        residual_returns = returns.subtract(market_return, axis=0)

        # ç´¯è®¡æ®‹å·®ï¼ˆè¿‡å»lookbackå¤©ï¼Œè·³è¿‡æœ€è¿‘skipå¤©ï¼‰
        residual_momentum = residual_returns.rolling(lookback).sum().shift(skip)

        return residual_momentum

    def momentum_52week_high(self):
        """
        52å‘¨æ–°é«˜å› å­ï¼šå½“å‰ä»·æ ¼ä¸è¿‡å»52å‘¨æœ€é«˜ä»·çš„æ¯”å€¼

        å¿ƒç†å­¦åŸºç¡€ï¼šæ¥è¿‘52å‘¨æ–°é«˜çš„è‚¡ç¥¨æœ‰çªç ´åŠ¨èƒ½
        """
        high_52w = self.price.rolling(252).max()
        ratio_to_high = self.price / high_52w
        return ratio_to_high

    def momentum_acceleration(self, short_period=20, long_period=60):
        """
        åŠ¨é‡åŠ é€Ÿåº¦ï¼šçŸ­æœŸåŠ¨é‡ - é•¿æœŸåŠ¨é‡

        è¡¡é‡åŠ¨é‡çš„å˜åŒ–è¶‹åŠ¿
        """
        short_mom = self.price / self.price.shift(short_period) - 1
        long_mom = self.price / self.price.shift(long_period) - 1
        acceleration = short_mom - long_mom
        return acceleration

# ä½¿ç”¨ç¤ºä¾‹
# price_data = pd.read_csv('prices.csv', index_col='date', parse_dates=True)
# mom_factors = MomentumFactors(price_data)
#
# # è®¡ç®—å„ç§åŠ¨é‡å› å­
# mom_20d = mom_factors.momentum_simple(lookback=20)
# mom_252d = mom_factors.momentum_skip_recent(lookback=252, skip=20)
# mom_residual = mom_factors.momentum_residual()
# mom_52w_high = mom_factors.momentum_52week_high()
```

**2. æ³¢åŠ¨ç‡å› å­**

```python
class VolatilityFactors:
    """æ³¢åŠ¨ç‡ç±»å› å­"""

    def __init__(self, price_data):
        self.price = price_data
        self.returns = price_data.pct_change()

    def volatility_historical(self, window=20):
        """
        å†å²æ³¢åŠ¨ç‡ï¼šè¿‡å»Nå¤©æ”¶ç›Šç‡çš„æ ‡å‡†å·®ï¼ˆå¹´åŒ–ï¼‰
        """
        vol = self.returns.rolling(window).std() * np.sqrt(252)
        return vol

    def volatility_idiosyncratic(self, window=60):
        """
        ç‰¹è´¨æ³¢åŠ¨ç‡ï¼šå»é™¤å¸‚åœºå½±å“åçš„æ³¢åŠ¨ç‡

        æ­¥éª¤ï¼š
        1. å¯¹å¸‚åœºæ”¶ç›Šç‡å›å½’ï¼Œå¾—åˆ°è´å¡”å’Œæ®‹å·®
        2. æ®‹å·®çš„æ ‡å‡†å·®å³ä¸ºç‰¹è´¨æ³¢åŠ¨ç‡
        """
        market_return = self.returns.mean(axis=1)

        idio_vols = pd.DataFrame(index=self.returns.index, columns=self.returns.columns)

        for stock in self.returns.columns:
            stock_returns = self.returns[stock]

            # æ»šåŠ¨çª—å£è®¡ç®—
            for i in range(window, len(stock_returns)):
                y = stock_returns.iloc[i-window:i].values
                x = market_return.iloc[i-window:i].values

                # å»é™¤NaN
                mask = ~(np.isnan(y) | np.isnan(x))
                if mask.sum() < 30:  # è‡³å°‘éœ€è¦30ä¸ªè§‚æµ‹å€¼
                    continue

                y_clean = y[mask]
                x_clean = x[mask]

                # çº¿æ€§å›å½’
                from scipy import stats
                slope, intercept, r_value, p_value, std_err = stats.linregress(x_clean, y_clean)

                # æ®‹å·®
                residuals = y_clean - (slope * x_clean + intercept)

                # ç‰¹è´¨æ³¢åŠ¨ç‡
                idio_vol = np.std(residuals) * np.sqrt(252)
                idio_vols.iloc[i, idio_vols.columns.get_loc(stock)] = idio_vol

        return idio_vols

    def volatility_downside(self, window=60, mar=0):
        """
        ä¸‹è¡Œæ³¢åŠ¨ç‡ï¼šåªè€ƒè™‘è´Ÿæ”¶ç›Šçš„æ³¢åŠ¨ç‡

        å‚æ•°:
            mar: Minimum Acceptable Returnï¼Œæœ€ä½å¯æ¥å—æ”¶ç›Šç‡
        """
        # åªä¿ç•™ä½äºmarçš„æ”¶ç›Š
        downside_returns = self.returns.copy()
        downside_returns[downside_returns > mar] = 0

        # è®¡ç®—ä¸‹è¡Œæ ‡å‡†å·®
        downside_vol = downside_returns.rolling(window).std() * np.sqrt(252)
        return downside_vol

    def beta(self, window=60):
        """
        è´å¡”å› å­ï¼šç›¸å¯¹å¸‚åœºçš„æ•æ„Ÿåº¦

        Beta = Cov(R_stock, R_market) / Var(R_market)
        """
        market_return = self.returns.mean(axis=1)

        betas = pd.DataFrame(index=self.returns.index, columns=self.returns.columns)

        for stock in self.returns.columns:
            # æ»šåŠ¨çª—å£è®¡ç®—åæ–¹å·®å’Œæ–¹å·®
            cov = self.returns[stock].rolling(window).cov(market_return)
            var = market_return.rolling(window).var()
            betas[stock] = cov / var

        return betas

# ä½¿ç”¨ç¤ºä¾‹
# vol_factors = VolatilityFactors(price_data)
# hist_vol = vol_factors.volatility_historical(window=20)
# idio_vol = vol_factors.volatility_idiosyncratic(window=60)
# down_vol = vol_factors.volatility_downside(window=60)
# beta = vol_factors.beta(window=60)
```

**3. é‡ä»·å› å­**

```python
class VolumeFactors:
    """é‡ä»·ç±»å› å­"""

    def __init__(self, price_data, volume_data):
        self.price = price_data
        self.volume = volume_data
        self.returns = price_data.pct_change()

    def turnover_rate(self, total_shares):
        """
        æ¢æ‰‹ç‡ï¼šæˆäº¤é‡ / æµé€šè‚¡æœ¬

        å‚æ•°:
            total_shares: æµé€šè‚¡æœ¬æ•°æ®
        """
        turnover = self.volume / total_shares
        return turnover

    def volume_price_correlation(self, window=20):
        """
        é‡ä»·ç›¸å…³æ€§ï¼šä»·æ ¼å˜åŒ–ä¸æˆäº¤é‡å˜åŒ–çš„ç›¸å…³ç³»æ•°

        æ­£ç›¸å…³ï¼šé‡ä»·é½å‡ï¼Œå¥åº·ä¸Šæ¶¨
        è´Ÿç›¸å…³ï¼šä»·æ¶¨é‡ç¼©æˆ–ä»·è·Œé‡å¢ï¼Œå¯èƒ½åè½¬
        """
        volume_change = self.volume.pct_change()

        corr = pd.DataFrame(index=self.price.index, columns=self.price.columns)

        for stock in self.price.columns:
            corr[stock] = self.returns[stock].rolling(window).corr(volume_change[stock])

        return corr

    def money_flow_20d(self):
        """
        20æ—¥èµ„é‡‘æµå‘ï¼š(close - open) / (high - low) * volume

        è¡¡é‡ä¸»åŠ¨ä¹°å…¥å’Œä¸»åŠ¨å–å‡ºçš„åŠ›é‡å¯¹æ¯”
        """
        # è¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦åˆ†æ—¶æ•°æ®
        # ä½¿ç”¨æ”¶ç›˜ä»·ä¸å¼€ç›˜ä»·çš„å…³ç³»è¿‘ä¼¼
        pass

    def volume_ratio(self, short=5, long=20):
        """
        é‡æ¯”ï¼šçŸ­æœŸå¹³å‡æˆäº¤é‡ / é•¿æœŸå¹³å‡æˆäº¤é‡

        é‡æ¯” > 1: æˆäº¤é‡æ”¾å¤§
        é‡æ¯” < 1: æˆäº¤é‡èç¼©
        """
        vol_ma_short = self.volume.rolling(short).mean()
        vol_ma_long = self.volume.rolling(long).mean()
        vol_ratio = vol_ma_short / vol_ma_long
        return vol_ratio

    def amihud_illiquidity(self, window=20):
        """
        AmihudéæµåŠ¨æ€§æŒ‡æ ‡ï¼š|æ”¶ç›Šç‡| / æˆäº¤é¢

        è¡¡é‡å•ä½æˆäº¤é¢å¯¹ä»·æ ¼çš„å†²å‡»
        å€¼è¶Šå¤§ï¼ŒæµåŠ¨æ€§è¶Šå·®
        """
        # æˆäº¤é¢ = æˆäº¤é‡ * ä»·æ ¼ï¼ˆç®€åŒ–ï¼‰
        amount = self.volume * self.price

        # éæµåŠ¨æ€§ = |æ”¶ç›Šç‡| / æˆäº¤é¢
        illiquidity = abs(self.returns) / amount

        # æ»šåŠ¨å¹³å‡
        illiquidity_avg = illiquidity.rolling(window).mean()

        return illiquidity_avg

# ä½¿ç”¨ç¤ºä¾‹
# vol_factors = VolumeFactors(price_data, volume_data)
# turnover = vol_factors.turnover_rate(total_shares)
# vp_corr = vol_factors.volume_price_correlation(window=20)
# vol_ratio = vol_factors.volume_ratio(short=5, long=20)
```

### â— 18.2.2 åŸºæœ¬é¢å› å­

åŸºæœ¬é¢å› å­åŸºäºå…¬å¸è´¢åŠ¡æ•°æ®ã€ç»è¥æŒ‡æ ‡æ„å»ºã€‚

```python
class FundamentalFactors:
    """åŸºæœ¬é¢å› å­"""

    def __init__(self, fundamental_data):
        """
        å‚æ•°:
            fundamental_data: DataFrame, åŒ…å«è´¢åŠ¡æ•°æ®
                - market_cap: å¸‚å€¼
                - pe_ratio: å¸‚ç›ˆç‡
                - pb_ratio: å¸‚å‡€ç‡
                - revenue: è¥ä¸šæ”¶å…¥
                - net_profit: å‡€åˆ©æ¶¦
                - total_assets: æ€»èµ„äº§
                - total_equity: è‚¡ä¸œæƒç›Š
                - operating_cash_flow: ç»è¥ç°é‡‘æµ
                ç­‰ç­‰
        """
        self.data = fundamental_data

    # ========== ä¼°å€¼å› å­ ==========

    def value_ep(self):
        """
        EP (Earnings to Price): ç›ˆåˆ©æ”¶ç›Šç‡ = 1/PE

        EPè¶Šé«˜ï¼Œä¼°å€¼è¶Šä½ï¼Œè¶Šæœ‰ä»·å€¼
        """
        ep = 1 / self.data['pe_ratio']
        return ep

    def value_bp(self):
        """
        BP (Book to Price): è´¦é¢å¸‚å€¼æ¯” = 1/PB

        ç»å…¸ä»·å€¼å› å­
        """
        bp = 1 / self.data['pb_ratio']
        return bp

    def value_sp(self):
        """
        SP (Sales to Price): é”€å”®æ”¶å…¥å¸‚å€¼æ¯”

        PSçš„å€’æ•°
        """
        sp = self.data['revenue'] / self.data['market_cap']
        return sp

    def value_cfp(self):
        """
        CFP (Cash Flow to Price): ç°é‡‘æµå¸‚å€¼æ¯”

        PCFçš„å€’æ•°ï¼Œæ¯”EPæ›´å¯é ï¼ˆç°é‡‘æµéš¾ä»¥æ“çºµï¼‰
        """
        cfp = self.data['operating_cash_flow'] / self.data['market_cap']
        return cfp

    def value_ev_ebitda(self):
        """
        EV/EBITDA: ä¼ä¸šä»·å€¼å€æ•°

        EV = å¸‚å€¼ + å‡€å€ºåŠ¡
        EBITDA = æ¯ç¨æŠ˜æ—§æ‘Šé”€å‰åˆ©æ¶¦

        è€ƒè™‘äº†å€ºåŠ¡ï¼Œæ¯”PEæ›´å…¨é¢
        """
        ev = self.data['market_cap'] + self.data['total_debt'] - self.data['cash']
        ev_ebitda = ev / self.data['ebitda']

        # è¿”å›å€’æ•°ï¼ˆå€¼è¶Šé«˜è¶Šæœ‰ä»·å€¼ï¼‰
        return 1 / ev_ebitda

    # ========== æˆé•¿å› å­ ==========

    def growth_revenue_yoy(self):
        """
        è¥æ”¶åŒæ¯”å¢é•¿ç‡
        """
        revenue_growth = self.data['revenue'].pct_change(4)  # å‡è®¾å­£åº¦æ•°æ®
        return revenue_growth

    def growth_profit_yoy(self):
        """
        å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿ç‡
        """
        profit_growth = self.data['net_profit'].pct_change(4)
        return profit_growth

    def growth_roe_change(self):
        """
        ROEå˜åŒ–ï¼šå½“æœŸROE - å»å¹´åŒæœŸROE

        è¡¡é‡ç›ˆåˆ©èƒ½åŠ›çš„æ”¹å–„
        """
        roe = self.data['net_profit'] / self.data['total_equity']
        roe_change = roe - roe.shift(4)
        return roe_change

    def growth_sustainable(self):
        """
        å¯æŒç»­å¢é•¿ç‡ï¼šROE * (1 - åˆ†çº¢ç‡)

        ç†è®ºä¸Šå…¬å¸åœ¨ä¸å¢åŠ è´¢åŠ¡æ æ†çš„æƒ…å†µä¸‹èƒ½è¾¾åˆ°çš„å¢é•¿ç‡
        """
        roe = self.data['net_profit'] / self.data['total_equity']
        payout_ratio = self.data['dividend'] / self.data['net_profit']
        sustainable_growth = roe * (1 - payout_ratio)
        return sustainable_growth

    # ========== è´¨é‡å› å­ ==========

    def quality_roe(self):
        """
        ROE (Return on Equity): å‡€èµ„äº§æ”¶ç›Šç‡

        å·´è²ç‰¹æœ€çœ‹é‡çš„æŒ‡æ ‡ä¹‹ä¸€
        """
        roe = self.data['net_profit'] / self.data['total_equity']
        return roe

    def quality_roa(self):
        """
        ROA (Return on Assets): æ€»èµ„äº§æ”¶ç›Šç‡

        è¡¡é‡èµ„äº§ä½¿ç”¨æ•ˆç‡
        """
        roa = self.data['net_profit'] / self.data['total_assets']
        return roa

    def quality_roic(self):
        """
        ROIC (Return on Invested Capital): æŠ•å…¥èµ„æœ¬å›æŠ¥ç‡

        ROIC = NOPAT / Invested Capital
        """
        nopat = self.data['operating_profit'] * (1 - self.data['tax_rate'])
        invested_capital = self.data['total_equity'] + self.data['total_debt']
        roic = nopat / invested_capital
        return roic

    def quality_gross_margin(self):
        """
        æ¯›åˆ©ç‡ï¼š(è¥ä¸šæ”¶å…¥ - è¥ä¸šæˆæœ¬) / è¥ä¸šæ”¶å…¥

        è¡¡é‡äº§å“ç«äº‰åŠ›å’Œå®šä»·èƒ½åŠ›
        """
        gross_margin = (self.data['revenue'] - self.data['cost']) / self.data['revenue']
        return gross_margin

    def quality_asset_turnover(self):
        """
        èµ„äº§å‘¨è½¬ç‡ï¼šè¥ä¸šæ”¶å…¥ / æ€»èµ„äº§

        è¡¡é‡èµ„äº§è¿è¥æ•ˆç‡
        """
        turnover = self.data['revenue'] / self.data['total_assets']
        return turnover

    # ========== è´¢åŠ¡å¥åº·å› å­ ==========

    def health_debt_to_equity(self):
        """
        è´Ÿå€ºè‚¡æƒæ¯”ï¼šæ€»è´Ÿå€º / è‚¡ä¸œæƒç›Š

        å€¼è¶Šå°ï¼Œè´¢åŠ¡è¶Šå¥åº·ï¼ˆè¿”å›è´Ÿå€¼ï¼Œä½¿å…¶ä¸å…¶ä»–å› å­æ–¹å‘ä¸€è‡´ï¼‰
        """
        debt_equity = self.data['total_debt'] / self.data['total_equity']
        return -debt_equity  # å–è´Ÿå€¼

    def health_current_ratio(self):
        """
        æµåŠ¨æ¯”ç‡ï¼šæµåŠ¨èµ„äº§ / æµåŠ¨è´Ÿå€º

        è¡¡é‡çŸ­æœŸå¿å€ºèƒ½åŠ›
        """
        current_ratio = self.data['current_assets'] / self.data['current_liabilities']
        return current_ratio

    def health_quick_ratio(self):
        """
        é€ŸåŠ¨æ¯”ç‡ï¼š(æµåŠ¨èµ„äº§ - å­˜è´§) / æµåŠ¨è´Ÿå€º

        æ¯”æµåŠ¨æ¯”ç‡æ›´ä¸¥æ ¼
        """
        quick_ratio = (self.data['current_assets'] - self.data['inventory']) / \
                      self.data['current_liabilities']
        return quick_ratio

    def health_accruals(self):
        """
        åº”è®¡é¡¹ç›®ï¼šå‡€åˆ©æ¶¦ - ç»è¥ç°é‡‘æµ

        åº”è®¡é¡¹ç›®ä½ï¼ˆç”šè‡³ä¸ºè´Ÿï¼‰è¯´æ˜ç›ˆåˆ©è´¨é‡é«˜
        """
        accruals = self.data['net_profit'] - self.data['operating_cash_flow']
        return -accruals  # å–è´Ÿå€¼ï¼Œè¶Šä½è¶Šå¥½

    # ========== å¤åˆå› å­ ==========

    def composite_piotroski_f_score(self):
        """
        Piotroski F-Score: ç»¼åˆè´¨é‡è¯„åˆ†ï¼ˆ0-9åˆ†ï¼‰

        9ä¸ªæŒ‡æ ‡ï¼Œæ¯ä¸ªæ»¡è¶³å¾—1åˆ†ï¼š

        ç›ˆåˆ©èƒ½åŠ›ï¼ˆ4ä¸ªï¼‰:
        1. ROA > 0
        2. ç»è¥ç°é‡‘æµ > 0
        3. ROAå¢é•¿
        4. ç»è¥ç°é‡‘æµ > å‡€åˆ©æ¶¦ï¼ˆåº”è®¡é¡¹ç›®ä¸ºè´Ÿï¼‰

        è´¢åŠ¡çŠ¶å†µï¼ˆ3ä¸ªï¼‰:
        5. é•¿æœŸè´Ÿå€ºç‡ä¸‹é™
        6. æµåŠ¨æ¯”ç‡ä¸Šå‡
        7. æ— å¢å‘ï¼ˆè‚¡æœ¬æœªå¢åŠ ï¼‰

        ç»è¥æ•ˆç‡ï¼ˆ2ä¸ªï¼‰:
        8. æ¯›åˆ©ç‡ä¸Šå‡
        9. èµ„äº§å‘¨è½¬ç‡ä¸Šå‡
        """
        score = 0

        # 1. ROA > 0
        roa = self.data['net_profit'] / self.data['total_assets']
        score += (roa > 0).astype(int)

        # 2. ç»è¥ç°é‡‘æµ > 0
        score += (self.data['operating_cash_flow'] > 0).astype(int)

        # 3. ROAå¢é•¿
        score += (roa > roa.shift(1)).astype(int)

        # 4. ç»è¥ç°é‡‘æµ > å‡€åˆ©æ¶¦
        score += (self.data['operating_cash_flow'] > self.data['net_profit']).astype(int)

        # 5-9çœç•¥ï¼Œå®é™…éœ€è¦æ›´å¤šæ•°æ®
        # ...

        return score

# ä½¿ç”¨ç¤ºä¾‹
# fund_factors = FundamentalFactors(fundamental_data)
#
# # ä¼°å€¼å› å­
# ep = fund_factors.value_ep()
# bp = fund_factors.value_bp()
#
# # æˆé•¿å› å­
# revenue_growth = fund_factors.growth_revenue_yoy()
# profit_growth = fund_factors.growth_profit_yoy()
#
# # è´¨é‡å› å­
# roe = fund_factors.quality_roe()
# gross_margin = fund_factors.quality_gross_margin()
#
# # è´¢åŠ¡å¥åº·
# debt_equity = fund_factors.health_debt_to_equity()
# current_ratio = fund_factors.health_current_ratio()
```

### â— 18.2.3 å¦ç±»å› å­

å¦ç±»å› å­åˆ©ç”¨éä¼ ç»Ÿæ•°æ®æºï¼Œæ˜¯è¿‘å¹´æ¥çš„ç ”ç©¶çƒ­ç‚¹ã€‚

```python
class AlternativeFactors:
    """å¦ç±»å› å­"""

    # ========== èˆ†æƒ…å› å­ ==========

    def sentiment_news(self, news_data):
        """
        æ–°é—»æƒ…æ„Ÿå› å­

        å‚æ•°:
            news_data: DataFrame, åŒ…å«stock_code, date, sentiment_score
                       sentiment_score: -1åˆ°1ï¼Œè´Ÿé¢åˆ°æ­£é¢
        """
        # è®¡ç®—æ¯åªè‚¡ç¥¨æ¯å¤©çš„å¹³å‡æƒ…æ„Ÿå¾—åˆ†
        sentiment = news_data.groupby(['date', 'stock_code'])['sentiment_score'].mean()
        sentiment = sentiment.unstack(level='stock_code')

        # æ»šåŠ¨å¹³å‡ï¼ˆå¹³æ»‘ï¼‰
        sentiment_ma = sentiment.rolling(5).mean()

        return sentiment_ma

    def sentiment_social_media(self, social_data):
        """
        ç¤¾äº¤åª’ä½“çƒ­åº¦å› å­

        å‚æ•°:
            social_data: DataFrame, åŒ…å«stock_code, date, mention_count
        """
        # æåŠæ¬¡æ•°
        mentions = social_data.groupby(['date', 'stock_code'])['mention_count'].sum()
        mentions = mentions.unstack(level='stock_code')

        # æ ‡å‡†åŒ–ï¼ˆç›¸å¯¹äºå†å²å¹³å‡ï¼‰
        mentions_normalized = (mentions - mentions.rolling(20).mean()) / \
                              mentions.rolling(20).std()

        return mentions_normalized

    # ========== åˆ†æå¸ˆå› å­ ==========

    def analyst_rating_change(self, rating_data):
        """
        åˆ†æå¸ˆè¯„çº§å˜åŒ–å› å­

        å‚æ•°:
            rating_data: DataFrame, åŒ…å«stock_code, date, rating
                        rating: 1-5ï¼Œ1=å¼ºçƒˆå–å‡ºï¼Œ5=å¼ºçƒˆä¹°å…¥
        """
        # è®¡ç®—æ¯åªè‚¡ç¥¨çš„å¹³å‡è¯„çº§
        avg_rating = rating_data.groupby(['date', 'stock_code'])['rating'].mean()
        avg_rating = avg_rating.unstack(level='stock_code')

        # è¯„çº§å˜åŒ–
        rating_change = avg_rating - avg_rating.shift(20)  # ç›¸å¯¹20å¤©å‰çš„å˜åŒ–

        return rating_change

    def analyst_eps_revision(self, eps_forecast_data):
        """
        åˆ†æå¸ˆEPSé¢„æµ‹ä¸Šè°ƒå› å­

        å‚æ•°:
            eps_forecast_data: DataFrame, åŒ…å«stock_code, date, eps_forecast
        """
        # æœ€æ–°é¢„æµ‹
        latest_forecast = eps_forecast_data.groupby(['date', 'stock_code'])['eps_forecast'].mean()
        latest_forecast = latest_forecast.unstack(level='stock_code')

        # é¢„æµ‹ä¿®æ­£å¹…åº¦
        revision = latest_forecast / latest_forecast.shift(60) - 1  # ç›¸å¯¹60å¤©å‰çš„ä¿®æ­£

        return revision

    # ========== è‚¡ä¸œå› å­ ==========

    def insider_trading(self, insider_data):
        """
        å†…éƒ¨äººäº¤æ˜“å› å­

        å‚æ•°:
            insider_data: DataFrame, åŒ…å«stock_code, date, net_purchase
                         net_purchase: å‡€ä¹°å…¥é‡‘é¢ï¼ˆä¹°å…¥-å–å‡ºï¼‰
        """
        net_purchase = insider_data.groupby(['date', 'stock_code'])['net_purchase'].sum()
        net_purchase = net_purchase.unstack(level='stock_code')

        # æ ‡å‡†åŒ–
        net_purchase_std = (net_purchase - net_purchase.rolling(60).mean()) / \
                           net_purchase.rolling(60).std()

        return net_purchase_std

    def institutional_ownership_change(self, ownership_data):
        """
        æœºæ„æŒä»“å˜åŒ–å› å­

        å‚æ•°:
            ownership_data: DataFrame, åŒ…å«stock_code, date, inst_ownership_pct
        """
        inst_own = ownership_data.set_index(['date', 'stock_code'])['inst_ownership_pct']
        inst_own = inst_own.unstack(level='stock_code')

        # æŒä»“å˜åŒ–ï¼ˆå­£åº¦ç¯æ¯”ï¼‰
        ownership_change = inst_own - inst_own.shift(60)  # çº¦ä¸€ä¸ªå­£åº¦

        return ownership_change

    # ========== äº‹ä»¶å› å­ ==========

    def ma_announcement(self, ma_data):
        """
        å¹¶è´­é‡ç»„å…¬å‘Šå› å­

        å‚æ•°:
            ma_data: DataFrame, åŒ…å«stock_code, announcement_date
        """
        # åˆ›å»ºå¹¶è´­äº‹ä»¶æŒ‡ç¤ºå™¨
        ma_indicator = pd.DataFrame(0, index=ma_data['announcement_date'].unique(),
                                    columns=ma_data['stock_code'].unique())

        for _, row in ma_data.iterrows():
            ma_indicator.loc[row['announcement_date'], row['stock_code']] = 1

        # äº‹ä»¶åçš„å½±å“çª—å£ï¼ˆä¾‹å¦‚ï¼Œäº‹ä»¶å20å¤©å†…éƒ½æ ‡è®°ä¸º1ï¼‰
        ma_window = ma_indicator.rolling(20, min_periods=1).max()

        return ma_window

# ä½¿ç”¨ç¤ºä¾‹
# alt_factors = AlternativeFactors()
#
# # èˆ†æƒ…å› å­
# news_sentiment = alt_factors.sentiment_news(news_data)
# social_heat = alt_factors.sentiment_social_media(social_data)
#
# # åˆ†æå¸ˆå› å­
# rating_change = alt_factors.analyst_rating_change(rating_data)
# eps_revision = alt_factors.analyst_eps_revision(eps_forecast_data)
#
# # è‚¡ä¸œå› å­
# insider = alt_factors.insider_trading(insider_data)
# inst_change = alt_factors.institutional_ownership_change(ownership_data)
```

### â— 18.2.4 å› å­è¡¨è¾¾å¼è®¾è®¡

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬å¸¸å¸¸éœ€è¦è®¾è®¡å¤æ‚çš„å› å­è¡¨è¾¾å¼ã€‚

```python
class FactorExpressionEngine:
    """å› å­è¡¨è¾¾å¼å¼•æ“ï¼ˆç±»ä¼¼Qlibçš„Alpha158ï¼‰"""

    def __init__(self, data):
        """
        å‚æ•°:
            data: DataFrame, åŒ…å«OHLCVæ•°æ®
        """
        self.close = data['close']
        self.open = data['open']
        self.high = data['high']
        self.low = data['low']
        self.volume = data['volume']

    # ========== åŸºç¡€ç®—å­ ==========

    def ts_mean(self, series, window):
        """æ—¶åºå¹³å‡"""
        return series.rolling(window).mean()

    def ts_std(self, series, window):
        """æ—¶åºæ ‡å‡†å·®"""
        return series.rolling(window).std()

    def ts_corr(self, x, y, window):
        """æ—¶åºç›¸å…³ç³»æ•°"""
        return x.rolling(window).corr(y)

    def ts_rank(self, series, window):
        """æ—¶åºæ’åï¼ˆç™¾åˆ†ä½ï¼‰"""
        return series.rolling(window).apply(
            lambda x: pd.Series(x).rank().iloc[-1] / len(x)
        )

    def ts_delta(self, series, period):
        """æ—¶åºå·®åˆ†"""
        return series - series.shift(period)

    def cs_rank(self, series):
        """æˆªé¢æ’å"""
        return series.rank(pct=True, axis=1)

    def cs_normalize(self, series):
        """æˆªé¢æ ‡å‡†åŒ–"""
        return (series - series.mean(axis=1, keepdims=True)) / \
               series.std(axis=1, keepdims=True)

    # ========== å¤åˆå› å­è¡¨è¾¾å¼ ==========

    def alpha001(self):
        """
        Alpha#001: (rank(Ts_ArgMax(SignedPower(((returns < 0) ? stddev(returns, 20) : close), 2.), 5)) - 0.5)

        ç®€åŒ–ç‰ˆï¼šæ”¶ç›Šç‡ä¸ºè´Ÿæ—¶ï¼Œç”¨æ³¢åŠ¨ç‡ï¼›ä¸ºæ­£æ—¶ç”¨ä»·æ ¼
        """
        returns = self.close.pct_change()
        vol = returns.rolling(20).std()

        condition = returns < 0
        signal = condition * vol + (~condition) * self.close

        # æ‰¾åˆ°è¿‡å»5å¤©çš„æœ€å¤§å€¼ä½ç½®
        argmax = signal.rolling(5).apply(lambda x: x.argmax())

        # æ’å
        factor = self.cs_rank(argmax) - 0.5
        return factor

    def alpha002(self):
        """
        Alpha#002: -1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6)

        æˆäº¤é‡å˜åŒ–ä¸æ—¥å†…æ”¶ç›Šç‡çš„è´Ÿç›¸å…³
        """
        volume_change = np.log(self.volume).diff(2)
        intraday_return = (self.close - self.open) / self.open

        rank_vol = self.cs_rank(volume_change)
        rank_ret = self.cs_rank(intraday_return)

        corr = rank_vol.rolling(6).corr(rank_ret)

        factor = -1 * corr
        return factor

    def alpha003(self):
        """
        Alpha#003: -1 * correlation(rank(open), rank(volume), 10)

        å¼€ç›˜ä»·ä¸æˆäº¤é‡çš„è´Ÿç›¸å…³
        """
        rank_open = self.cs_rank(self.open)
        rank_vol = self.cs_rank(self.volume)

        corr = rank_open.rolling(10).corr(rank_vol)

        factor = -1 * corr
        return factor

    def alpha_custom_momentum_volatility(self):
        """
        è‡ªå®šä¹‰ï¼šåŠ¨é‡è°ƒæ•´åçš„æ³¢åŠ¨ç‡å› å­

        é€»è¾‘ï¼š
        - é«˜åŠ¨é‡ + ä½æ³¢åŠ¨ç‡ = ç¨³å¥ä¸Šæ¶¨ï¼ˆå¥½ï¼‰
        - ä½åŠ¨é‡ + é«˜æ³¢åŠ¨ç‡ = ä¸ç¨³å®šï¼ˆå·®ï¼‰
        """
        # 20æ—¥åŠ¨é‡
        momentum = self.close / self.close.shift(20) - 1

        # 20æ—¥æ³¢åŠ¨ç‡
        returns = self.close.pct_change()
        volatility = returns.rolling(20).std()

        # åŠ¨é‡/æ³¢åŠ¨ç‡
        factor = momentum / (volatility + 1e-6)

        # æˆªé¢æ ‡å‡†åŒ–
        factor = self.cs_normalize(factor)

        return factor

    def alpha_custom_value_momentum(self):
        """
        è‡ªå®šä¹‰ï¼šä»·å€¼åŠ¨é‡ç»„åˆå› å­

        é€»è¾‘ï¼š
        - ä½ä¼°å€¼ï¼ˆé«˜EPï¼‰ + æ­£åŠ¨é‡ = ä»·å€¼+åŠ¨é‡åŒå‡»ï¼ˆå¥½ï¼‰
        """
        # éœ€è¦é¢å¤–çš„è´¢åŠ¡æ•°æ®ï¼ˆè¿™é‡Œç®€åŒ–ï¼Œç”¨ä»·æ ¼å€’æ•°æ¨¡æ‹ŸEPï¼‰
        ep = 1 / self.close  # ç®€åŒ–çš„EP

        # 60æ—¥åŠ¨é‡
        momentum = self.close / self.close.shift(60) - 1

        # ç»„åˆï¼šEP * (1 + momentum)
        factor = ep * (1 + momentum)

        # æˆªé¢æ ‡å‡†åŒ–
        factor = self.cs_normalize(factor)

        return factor

# ä½¿ç”¨ç¤ºä¾‹
# engine = FactorExpressionEngine(ohlcv_data)
#
# # è®¡ç®—å„ç§å› å­
# alpha1 = engine.alpha001()
# alpha2 = engine.alpha002()
# alpha3 = engine.alpha003()
#
# # è‡ªå®šä¹‰å› å­
# mom_vol = engine.alpha_custom_momentum_volatility()
# value_mom = engine.alpha_custom_value_momentum()
```

**å› å­è®¾è®¡çš„ç»éªŒæ³•åˆ™**ï¼š

1. **ç®€å•ä¼˜äºå¤æ‚**ï¼šå¤æ‚å› å­å®¹æ˜“è¿‡æ‹Ÿåˆ
2. **æœ‰ç»æµå­¦ç›´è§‰**ï¼šèƒ½è§£é‡Šä¸ºä»€ä¹ˆæœ‰æ•ˆ
3. **ç¨³å¥æ€§**ï¼šåœ¨ä¸åŒæ—¶æœŸã€ä¸åŒå¸‚åœºéƒ½æœ‰æ•ˆ
4. **ä½ç›¸å…³æ€§**ï¼šä¸å·²æœ‰å› å­ç›¸å…³æ€§ä½ï¼Œæä¾›æ–°ä¿¡æ¯
5. **å¯äº¤æ˜“æ€§**ï¼šè€ƒè™‘äº¤æ˜“æˆæœ¬ã€å®¹é‡é™åˆ¶

---

## 18.3 å› å­æœ‰æ•ˆæ€§æ£€éªŒ

æ„å»ºå› å­åï¼Œå¿…é¡»ä¸¥æ ¼æ£€éªŒå…¶æœ‰æ•ˆæ€§ã€‚

### â— 18.3.1 ICï¼ˆä¿¡æ¯ç³»æ•°ï¼‰åˆ†æ

ICæ˜¯å› å­å€¼ä¸æœªæ¥æ”¶ç›Šç‡çš„ç›¸å…³ç³»æ•°ï¼Œæ˜¯æœ€å¸¸ç”¨çš„å› å­è¯„ä»·æŒ‡æ ‡ã€‚

```python
class ICAnalyzer:
    """ICåˆ†æå™¨"""

    def __init__(self, factor_data, return_data):
        """
        å‚æ•°:
            factor_data: DataFrame, å› å­å€¼ï¼Œindex=date, columns=stock_code
            return_data: DataFrame, æ”¶ç›Šç‡ï¼Œindex=date, columns=stock_code
        """
        self.factor = factor_data
        self.returns = return_data

    def calculate_ic(self, periods=[1, 5, 10, 20]):
        """
        è®¡ç®—ä¸åŒæŒæœ‰æœŸçš„IC

        å‚æ•°:
            periods: list, æŒæœ‰æœŸåˆ—è¡¨ï¼ˆå¤©æ•°ï¼‰

        è¿”å›:
            dict, æ¯ä¸ªæŒæœ‰æœŸçš„ICæ—¶é—´åºåˆ—
        """
        ic_results = {}

        for period in periods:
            # æœªæ¥periodå¤©çš„æ”¶ç›Šç‡
            future_returns = self.returns.shift(-period)

            # è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹çš„IC
            ic_series = []

            for date in self.factor.index:
                if date not in future_returns.index:
                    continue

                factor_values = self.factor.loc[date]
                return_values = future_returns.loc[date]

                # å»é™¤NaN
                valid_mask = factor_values.notna() & return_values.notna()

                if valid_mask.sum() < 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆæ ·æœ¬
                    continue

                # è®¡ç®—ç›¸å…³ç³»æ•°ï¼ˆSpearmanæˆ–Pearsonï¼‰
                ic_pearson = factor_values[valid_mask].corr(return_values[valid_mask])
                ic_spearman = factor_values[valid_mask].corr(return_values[valid_mask], method='spearman')

                ic_series.append({
                    'date': date,
                    'ic_pearson': ic_pearson,
                    'ic_spearman': ic_spearman,
                    'n_stocks': valid_mask.sum()
                })

            ic_df = pd.DataFrame(ic_series).set_index('date')
            ic_results[f'{period}d'] = ic_df

        return ic_results

    def ic_statistics(self, ic_results):
        """
        è®¡ç®—ICç»Ÿè®¡æŒ‡æ ‡

        è¿”å›:
            DataFrame, ICç»Ÿè®¡æ‘˜è¦
        """
        stats = []

        for period, ic_df in ic_results.items():
            ic_values = ic_df['ic_pearson']

            stat = {
                'æŒæœ‰æœŸ': period,
                'ICå‡å€¼': ic_values.mean(),
                'ICæ ‡å‡†å·®': ic_values.std(),
                'ICIR': ic_values.mean() / ic_values.std() if ic_values.std() > 0 else 0,
                'ICèƒœç‡': (ic_values > 0).mean(),
                'ICç»å¯¹å€¼>0.02å æ¯”': (abs(ic_values) > 0.02).mean(),
                'tç»Ÿè®¡é‡': ic_values.mean() / ic_values.std() * np.sqrt(len(ic_values)),
            }
            stats.append(stat)

        stats_df = pd.DataFrame(stats)
        return stats_df

    def plot_ic_analysis(self, ic_results, save_path='ic_analysis.png'):
        """
        å¯è§†åŒ–ICåˆ†æ
        """
        import matplotlib.pyplot as plt
        import seaborn as sns

        n_periods = len(ic_results)
        fig, axes = plt.subplots(n_periods, 2, figsize=(14, 4*n_periods))

        if n_periods == 1:
            axes = axes.reshape(1, -1)

        for i, (period, ic_df) in enumerate(ic_results.items()):
            ic_values = ic_df['ic_pearson']

            # å­å›¾1ï¼šICæ—¶é—´åºåˆ—
            axes[i, 0].plot(ic_df.index, ic_values, alpha=0.7, linewidth=1)
            axes[i, 0].axhline(y=0, color='r', linestyle='--', linewidth=1)
            axes[i, 0].axhline(y=ic_values.mean(), color='g', linestyle='--',
                             linewidth=1.5, label=f'å‡å€¼={ic_values.mean():.4f}')
            axes[i, 0].fill_between(ic_df.index, 0, ic_values,
                                   where=(ic_values > 0), alpha=0.3, color='green')
            axes[i, 0].fill_between(ic_df.index, 0, ic_values,
                                   where=(ic_values <= 0), alpha=0.3, color='red')
            axes[i, 0].set_title(f'ICæ—¶é—´åºåˆ— ({period})')
            axes[i, 0].set_ylabel('IC')
            axes[i, 0].legend()
            axes[i, 0].grid(True, alpha=0.3)

            # å­å›¾2ï¼šICåˆ†å¸ƒç›´æ–¹å›¾
            axes[i, 1].hist(ic_values, bins=50, alpha=0.7, edgecolor='black')
            axes[i, 1].axvline(x=0, color='r', linestyle='--', linewidth=1)
            axes[i, 1].axvline(x=ic_values.mean(), color='g', linestyle='--',
                             linewidth=1.5, label=f'å‡å€¼={ic_values.mean():.4f}')
            axes[i, 1].set_title(f'ICåˆ†å¸ƒ ({period})')
            axes[i, 1].set_xlabel('IC')
            axes[i, 1].set_ylabel('é¢‘æ•°')
            axes[i, 1].legend()
            axes[i, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

    def ic_decay_analysis(self, max_period=20):
        """
        ICè¡°å‡åˆ†æï¼šè§‚å¯ŸICéšæŒæœ‰æœŸçš„å˜åŒ–

        å¥½çš„å› å­åº”è¯¥åœ¨è¾ƒé•¿æŒæœ‰æœŸå†…ä»ä¿æŒä¸€å®šIC
        """
        periods = list(range(1, max_period+1))
        ic_means = []
        ic_stds = []

        for period in periods:
            future_returns = self.returns.shift(-period)

            # è®¡ç®—å¹³å‡IC
            ics = []
            for date in self.factor.index:
                if date not in future_returns.index:
                    continue

                factor_values = self.factor.loc[date]
                return_values = future_returns.loc[date]

                valid_mask = factor_values.notna() & return_values.notna()
                if valid_mask.sum() < 10:
                    continue

                ic = factor_values[valid_mask].corr(return_values[valid_mask])
                ics.append(ic)

            ic_means.append(np.mean(ics))
            ic_stds.append(np.std(ics))

        # å¯è§†åŒ–
        plt.figure(figsize=(10, 6))
        plt.plot(periods, ic_means, 'o-', label='ICå‡å€¼', linewidth=2)
        plt.fill_between(periods,
                        np.array(ic_means) - np.array(ic_stds),
                        np.array(ic_means) + np.array(ic_stds),
                        alpha=0.3, label='Â±1æ ‡å‡†å·®')
        plt.xlabel('æŒæœ‰æœŸï¼ˆå¤©ï¼‰')
        plt.ylabel('IC')
        plt.title('ICè¡°å‡åˆ†æ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.axhline(y=0, color='r', linestyle='--', linewidth=1)
        plt.savefig('ic_decay.png', dpi=300, bbox_inches='tight')
        plt.show()

        return pd.DataFrame({'period': periods, 'ic_mean': ic_means, 'ic_std': ic_stds})

# ä½¿ç”¨ç¤ºä¾‹
# analyzer = ICAnalyzer(factor_data, return_data)
#
# # è®¡ç®—IC
# ic_results = analyzer.calculate_ic(periods=[1, 5, 10, 20])
#
# # ICç»Ÿè®¡
# ic_stats = analyzer.ic_statistics(ic_results)
# print(ic_stats)
#
# # å¯è§†åŒ–
# analyzer.plot_ic_analysis(ic_results)
#
# # ICè¡°å‡
# ic_decay = analyzer.ic_decay_analysis(max_period=60)
```

**ICè¯„åˆ¤æ ‡å‡†**ï¼š

| æŒ‡æ ‡ | ä¼˜ç§€ | è‰¯å¥½ | ä¸€èˆ¬ | è¾ƒå·® |
|------|------|------|------|------|
| ICå‡å€¼ | \|IC\| > 0.05 | 0.03 < \|IC\| â‰¤ 0.05 | 0.02 < \|IC\| â‰¤ 0.03 | \|IC\| â‰¤ 0.02 |
| ICIR | > 0.5 | 0.3 ~ 0.5 | 0.1 ~ 0.3 | < 0.1 |
| ICèƒœç‡ | > 60% | 55% ~ 60% | 52% ~ 55% | < 52% |
| tç»Ÿè®¡é‡ | > 3 | 2 ~ 3 | 1.5 ~ 2 | < 1.5 |

### â— 18.3.2 åˆ†ç»„å›æµ‹

åˆ†ç»„å›æµ‹æ˜¯æ£€éªŒå› å­å•è°ƒæ€§å’Œæ”¶ç›Šå·®å¼‚çš„é‡è¦æ–¹æ³•ã€‚

```python
class GroupBacktest:
    """åˆ†ç»„å›æµ‹"""

    def __init__(self, factor_data, return_data, price_data):
        self.factor = factor_data
        self.returns = return_data
        self.price = price_data

    def run_group_backtest(self, n_groups=10, holding_period=20, quantile_method='equal_freq'):
        """
        è¿è¡Œåˆ†ç»„å›æµ‹

        å‚æ•°:
            n_groups: åˆ†ç»„æ•°é‡
            holding_period: æŒæœ‰æœŸï¼ˆå¤©ï¼‰
            quantile_method: 'equal_freq'ç­‰é¢‘åˆ†ç»„ æˆ– 'equal_width'ç­‰å®½åˆ†ç»„

        è¿”å›:
            dict, åŒ…å«å„ç»„çš„å‡€å€¼æ›²çº¿ã€æ”¶ç›Šç»Ÿè®¡ç­‰
        """
        # åˆå§‹åŒ–å„ç»„å‡€å€¼
        group_nav = {i: [1.0] for i in range(n_groups)}
        group_positions = {i: [] for i in range(n_groups)}

        # è·å–è°ƒä»“æ—¥æœŸ
        rebalance_dates = self.factor.index[::holding_period]

        for date in rebalance_dates:
            if date not in self.factor.index:
                continue

            # å½“å‰å› å­å€¼
            current_factor = self.factor.loc[date].dropna()

            if len(current_factor) < n_groups:
                continue

            # åˆ†ç»„
            if quantile_method == 'equal_freq':
                # ç­‰é¢‘åˆ†ç»„ï¼šæ¯ç»„è‚¡ç¥¨æ•°é‡ç›¸ç­‰
                groups = pd.qcut(current_factor, n_groups, labels=False, duplicates='drop')
            else:
                # ç­‰å®½åˆ†ç»„ï¼šå› å­å€¼åŒºé—´ç›¸ç­‰
                groups = pd.cut(current_factor, n_groups, labels=False, duplicates='drop')

            # è®¡ç®—æ¯ç»„åœ¨æŒæœ‰æœŸå†…çš„æ”¶ç›Š
            end_date_idx = self.factor.index.get_loc(date) + holding_period
            if end_date_idx >= len(self.factor.index):
                break

            period_returns = self.returns.iloc[
                self.factor.index.get_loc(date):end_date_idx
            ]

            # æ¯ç»„çš„æ”¶ç›Š
            for group_id in range(n_groups):
                stocks_in_group = groups[groups == group_id].index.tolist()

                if len(stocks_in_group) > 0:
                    # ç­‰æƒç»„åˆæ”¶ç›Š
                    group_return = period_returns[stocks_in_group].mean(axis=1).sum()

                    # æ›´æ–°å‡€å€¼
                    group_nav[group_id].append(group_nav[group_id][-1] * (1 + group_return))

                    # è®°å½•æŒä»“
                    group_positions[group_id].append({
                        'date': date,
                        'stocks': stocks_in_group,
                        'factor_mean': current_factor[stocks_in_group].mean(),
                        'n_stocks': len(stocks_in_group)
                    })

        # è½¬æ¢ä¸ºDataFrame
        nav_df = pd.DataFrame(group_nav)
        nav_df.index = rebalance_dates[:len(nav_df)]

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        stats = self.calculate_group_statistics(nav_df)

        # å¯è§†åŒ–
        self.plot_group_backtest(nav_df, stats)

        return {
            'nav': nav_df,
            'stats': stats,
            'positions': group_positions
        }

    def calculate_group_statistics(self, nav_df):
        """è®¡ç®—å„ç»„ç»Ÿè®¡æŒ‡æ ‡"""
        stats = []

        for group_id in nav_df.columns:
            nav_series = nav_df[group_id]
            returns = nav_series.pct_change().dropna()

            stat = {
                'åˆ†ç»„': f'G{group_id}',
                'ç´¯è®¡æ”¶ç›Š': nav_series.iloc[-1] - 1,
                'å¹´åŒ–æ”¶ç›Š': (nav_series.iloc[-1]) ** (252 / len(nav_series)) - 1,
                'å¹´åŒ–æ³¢åŠ¨': returns.std() * np.sqrt(252 / 20),  # å‡è®¾20å¤©è°ƒä»“ä¸€æ¬¡
                'å¤æ™®æ¯”ç‡': (returns.mean() - 0.03/252*20) / returns.std() * np.sqrt(252/20) if returns.std() > 0 else 0,
                'æœ€å¤§å›æ’¤': (nav_series / nav_series.cummax() - 1).min(),
                'èƒœç‡': (returns > 0).mean(),
            }
            stats.append(stat)

        stats_df = pd.DataFrame(stats)
        return stats_df

    def plot_group_backtest(self, nav_df, stats_df, save_path='group_backtest.png'):
        """å¯è§†åŒ–åˆ†ç»„å›æµ‹ç»“æœ"""
        import matplotlib.pyplot as plt
        from matplotlib import cm

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # å­å›¾1ï¼šå‡€å€¼æ›²çº¿
        colors = cm.RdYlGn(np.linspace(0, 1, len(nav_df.columns)))

        for i, col in enumerate(nav_df.columns):
            axes[0, 0].plot(nav_df.index, nav_df[col], label=f'G{col}',
                          color=colors[i], linewidth=2)

        axes[0, 0].set_title('å„ç»„å‡€å€¼æ›²çº¿', fontsize=14, fontweight='bold')
        axes[0, 0].set_ylabel('å‡€å€¼')
        axes[0, 0].legend(ncol=2)
        axes[0, 0].grid(True, alpha=0.3)

        # å­å›¾2ï¼šç´¯è®¡æ”¶ç›ŠæŸ±çŠ¶å›¾
        cumulative_returns = stats_df['ç´¯è®¡æ”¶ç›Š'].values
        colors_bar = ['green' if x > 0 else 'red' for x in cumulative_returns]

        axes[0, 1].bar(range(len(cumulative_returns)), cumulative_returns * 100,
                      color=colors_bar, alpha=0.7, edgecolor='black')
        axes[0, 1].set_title('å„ç»„ç´¯è®¡æ”¶ç›Š', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('åˆ†ç»„ (G0=æœ€ä½å› å­, G9=æœ€é«˜å› å­)')
        axes[0, 1].set_ylabel('ç´¯è®¡æ”¶ç›Š (%)')
        axes[0, 1].axhline(y=0, color='black', linestyle='-', linewidth=0.8)
        axes[0, 1].grid(True, alpha=0.3, axis='y')

        # å­å›¾3ï¼šå¤æ™®æ¯”ç‡
        sharpe_ratios = stats_df['å¤æ™®æ¯”ç‡'].values

        axes[1, 0].bar(range(len(sharpe_ratios)), sharpe_ratios,
                      color='steelblue', alpha=0.7, edgecolor='black')
        axes[1, 0].set_title('å„ç»„å¤æ™®æ¯”ç‡', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('åˆ†ç»„')
        axes[1, 0].set_ylabel('å¤æ™®æ¯”ç‡')
        axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.8)
        axes[1, 0].grid(True, alpha=0.3, axis='y')

        # å­å›¾4ï¼šå¤šç©ºç»„åˆï¼ˆG9 - G0ï¼‰
        long_short_nav = nav_df.iloc[:, -1] - nav_df.iloc[:, 0] + 1

        axes[1, 1].plot(nav_df.index, long_short_nav, color='purple', linewidth=2.5)
        axes[1, 1].fill_between(nav_df.index, 1, long_short_nav,
                               where=(long_short_nav >= 1), alpha=0.3, color='green')
        axes[1, 1].fill_between(nav_df.index, 1, long_short_nav,
                               where=(long_short_nav < 1), alpha=0.3, color='red')
        axes[1, 1].set_title('å¤šç©ºç»„åˆå‡€å€¼ (G9 - G0)', fontsize=14, fontweight='bold')
        axes[1, 1].set_ylabel('å‡€å€¼')
        axes[1, 1].axhline(y=1, color='black', linestyle='--', linewidth=1)
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        # æ‰“å°ç»Ÿè®¡è¡¨
        print("\n" + "=" * 80)
        print(" " * 30 + "åˆ†ç»„å›æµ‹ç»Ÿè®¡")
        print("=" * 80)
        print(stats_df.to_string(index=False))
        print("=" * 80)

        # å•è°ƒæ€§æ£€éªŒ
        monotonicity = self.check_monotonicity(stats_df['ç´¯è®¡æ”¶ç›Š'].values)
        print(f"\nå•è°ƒæ€§æ£€éªŒ: {monotonicity}")

        # å¤šç©ºæ”¶ç›Š
        long_short_return = stats_df.iloc[-1]['ç´¯è®¡æ”¶ç›Š'] - stats_df.iloc[0]['ç´¯è®¡æ”¶ç›Š']
        print(f"å¤šç©ºç»„åˆæ”¶ç›Š: {long_short_return:.2%}")
        print("=" * 80)

    def check_monotonicity(self, values):
        """
        æ£€æŸ¥å•è°ƒæ€§

        å®Œå…¨å•è°ƒï¼šç›¸é‚»ç»„ä¹‹é—´éƒ½ä¿æŒå•è°ƒ
        åŸºæœ¬å•è°ƒï¼šå¤§éƒ¨åˆ†ç›¸é‚»ç»„ä¿æŒå•è°ƒ
        """
        n = len(values)
        increasing = sum([values[i+1] > values[i] for i in range(n-1)])
        decreasing = sum([values[i+1] < values[i] for i in range(n-1)])

        if increasing == n - 1:
            return "âœ… å®Œå…¨å•è°ƒé€’å¢"
        elif decreasing == n - 1:
            return "âœ… å®Œå…¨å•è°ƒé€’å‡"
        elif increasing >= (n-1) * 0.7:
            return "âš ï¸  åŸºæœ¬å•è°ƒé€’å¢"
        elif decreasing >= (n-1) * 0.7:
            return "âš ï¸  åŸºæœ¬å•è°ƒé€’å‡"
        else:
            return "âŒ ä¸å•è°ƒ"

# ä½¿ç”¨ç¤ºä¾‹
# gb = GroupBacktest(factor_data, return_data, price_data)
# result = gb.run_group_backtest(n_groups=10, holding_period=20)
```

### â— 18.3.3 å› å­è¡°å‡æµ‹è¯•

æµ‹è¯•å› å­çš„é¢„æµ‹èƒ½åŠ›éšæ—¶é—´çš„è¡°å‡æƒ…å†µã€‚

```python
def factor_decay_test(factor_data, return_data, max_lag=20):
    """
    å› å­è¡°å‡æµ‹è¯•

    æµ‹è¯•å› å­å¯¹ä¸åŒæœªæ¥æœŸæ•°æ”¶ç›Šç‡çš„é¢„æµ‹èƒ½åŠ›

    å‚æ•°:
        factor_data: DataFrame, å› å­å€¼
        return_data: DataFrame, æ—¥æ”¶ç›Šç‡
        max_lag: int, æœ€å¤§æ»åæœŸæ•°

    è¿”å›:
        DataFrame, ä¸åŒæ»åæœŸçš„ICå’Œæ”¶ç›Šå·®å¼‚
    """
    results = []

    for lag in range(1, max_lag + 1):
        # æœªæ¥lagå¤©çš„æ”¶ç›Šç‡
        future_return = return_data.rolling(lag).sum().shift(-lag)

        # è®¡ç®—IC
        ics = []
        top_bottom_diffs = []

        for date in factor_data.index:
            if date not in future_return.index:
                continue

            factor_values = factor_data.loc[date]
            return_values = future_return.loc[date]

            valid_mask = factor_values.notna() & return_values.notna()

            if valid_mask.sum() < 20:
                continue

            # IC
            ic = factor_values[valid_mask].corr(return_values[valid_mask])
            ics.append(ic)

            # åˆ†æˆ10ç»„ï¼Œè®¡ç®—top-bottomæ”¶ç›Šå·®
            deciles = pd.qcut(factor_values[valid_mask], 10, labels=False, duplicates='drop')
            top_return = return_values[valid_mask][deciles == 9].mean()
            bottom_return = return_values[valid_mask][deciles == 0].mean()
            top_bottom_diffs.append(top_return - bottom_return)

        results.append({
            'lag': lag,
            'mean_ic': np.mean(ics),
            'ic_ir': np.mean(ics) / np.std(ics) if np.std(ics) > 0 else 0,
            'mean_top_bottom_diff': np.mean(top_bottom_diffs),
            'n_samples': len(ics)
        })

    results_df = pd.DataFrame(results)

    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # ICè¡°å‡
    axes[0].plot(results_df['lag'], results_df['mean_ic'], 'o-', linewidth=2, markersize=6)
    axes[0].set_xlabel('æŒæœ‰æœŸï¼ˆå¤©ï¼‰')
    axes[0].set_ylabel('å¹³å‡IC')
    axes[0].set_title('ICè¡°å‡æ›²çº¿')
    axes[0].grid(True, alpha=0.3)
    axes[0].axhline(y=0, color='r', linestyle='--')

    # Top-Bottomæ”¶ç›Šå·®è¡°å‡
    axes[1].plot(results_df['lag'], results_df['mean_top_bottom_diff'] * 100,
                'o-', linewidth=2, markersize=6, color='green')
    axes[1].set_xlabel('æŒæœ‰æœŸï¼ˆå¤©ï¼‰')
    axes[1].set_ylabel('Top-Bottomæ”¶ç›Šå·® (%)')
    axes[1].set_title('åˆ†ç»„æ”¶ç›Šå·®è¡°å‡æ›²çº¿')
    axes[1].grid(True, alpha=0.3)
    axes[1].axhline(y=0, color='r', linestyle='--')

    plt.tight_layout()
    plt.savefig('factor_decay.png', dpi=300, bbox_inches='tight')
    plt.show()

    print("\nå› å­è¡°å‡æµ‹è¯•ç»“æœ:")
    print(results_df.to_string(index=False))

    return results_df

# ç†æƒ³çš„å› å­ï¼š
# - ICåœ¨çŸ­æœŸå†…ï¼ˆ1-5å¤©ï¼‰è¾ƒé«˜
# - ICéšæ—¶é—´é€æ¸è¡°å‡ï¼Œä½†åœ¨è¾ƒé•¿æœŸï¼ˆ20å¤©ï¼‰ä»ä¿æŒä¸€å®šæ°´å¹³
# - è¿™è¡¨æ˜å› å­æ—¢æœ‰çŸ­æœŸé¢„æµ‹èƒ½åŠ›ï¼Œåˆæœ‰ä¸­æœŸç¨³å®šæ€§
```

### â— 18.3.4 å› å­æ­£äº¤åŒ–

æ¶ˆé™¤å› å­ä¹‹é—´çš„å…±çº¿æ€§ï¼Œæå–ç‹¬ç«‹çš„alphaä¿¡å·ã€‚

```python
class FactorOrthogonalization:
    """å› å­æ­£äº¤åŒ–"""

    def __init__(self, factor_dict):
        """
        å‚æ•°:
            factor_dict: dict, {å› å­å: å› å­DataFrame}
        """
        self.factors = factor_dict

    def calculate_factor_correlation(self):
        """
        è®¡ç®—å› å­ä¹‹é—´çš„ç›¸å…³ç³»æ•°çŸ©é˜µ
        """
        # å°†æ‰€æœ‰å› å­åˆå¹¶æˆä¸€ä¸ªå¤§DataFrame
        combined = pd.DataFrame()

        for name, factor_df in self.factors.items():
            # å±•å¹³æˆä¸€ç»´
            flat_values = factor_df.values.flatten()
            combined[name] = flat_values

        # å»é™¤NaN
        combined = combined.dropna()

        # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
        corr_matrix = combined.corr()

        # å¯è§†åŒ–
        plt.figure(figsize=(10, 8))
        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdYlGn_r',
                   center=0, vmin=-1, vmax=1, square=True,
                   linewidths=0.5, cbar_kws={'shrink': 0.8})
        plt.title('å› å­ç›¸å…³ç³»æ•°çŸ©é˜µ', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig('factor_correlation.png', dpi=300, bbox_inches='tight')
        plt.show()

        return corr_matrix

    def orthogonalize_single(self, target_factor, base_factors):
        """
        å¯¹å•ä¸ªå› å­è¿›è¡Œæ­£äº¤åŒ–

        æ–¹æ³•ï¼šå¯¹base_factorså›å½’ï¼Œä¿ç•™æ®‹å·®

        å‚æ•°:
            target_factor: DataFrame, ç›®æ ‡å› å­
            base_factors: list of DataFrame, åŸºå‡†å› å­åˆ—è¡¨

        è¿”å›:
            DataFrame, æ­£äº¤åŒ–åçš„å› å­
        """
        from sklearn.linear_model import LinearRegression

        orthogonal_factor = target_factor.copy()

        for date in target_factor.index:
            # å½“å‰æ—¥æœŸçš„å› å­å€¼
            y = target_factor.loc[date].values.reshape(-1, 1)

            # åŸºå‡†å› å­
            X_list = []
            for base_factor in base_factors:
                if date in base_factor.index:
                    X_list.append(base_factor.loc[date].values)

            if len(X_list) == 0:
                continue

            X = np.column_stack(X_list)

            # å»é™¤NaN
            valid_mask = ~(np.isnan(y).flatten() | np.any(np.isnan(X), axis=1))

            if valid_mask.sum() < 10:
                continue

            y_valid = y[valid_mask]
            X_valid = X[valid_mask]

            # çº¿æ€§å›å½’
            model = LinearRegression()
            model.fit(X_valid, y_valid)

            # æ®‹å·®
            residuals = y_valid - model.predict(X_valid)

            # æ›´æ–°æ­£äº¤åŒ–å› å­
            orthogonal_factor.loc[date, :] = np.nan
            orthogonal_factor.loc[date, target_factor.columns[valid_mask]] = residuals.flatten()

        return orthogonal_factor

    def schmidt_orthogonalization(self, factor_order=None):
        """
        Schmidtæ­£äº¤åŒ–ï¼šæŒ‰æŒ‡å®šé¡ºåºå¯¹å› å­è¿›è¡Œæ­£äº¤åŒ–

        å‚æ•°:
            factor_order: list, å› å­åç§°åˆ—è¡¨ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰
                         ç¬¬ä¸€ä¸ªå› å­ä¿æŒä¸å˜
                         ç¬¬äºŒä¸ªå› å­å¯¹ç¬¬ä¸€ä¸ªæ­£äº¤åŒ–
                         ç¬¬ä¸‰ä¸ªå› å­å¯¹å‰ä¸¤ä¸ªæ­£äº¤åŒ–
                         ...

        è¿”å›:
            dict, æ­£äº¤åŒ–åçš„å› å­
        """
        if factor_order is None:
            factor_order = list(self.factors.keys())

        orthogonal_factors = {}

        for i, factor_name in enumerate(factor_order):
            if i == 0:
                # ç¬¬ä¸€ä¸ªå› å­ä¿æŒä¸å˜
                orthogonal_factors[factor_name] = self.factors[factor_name]
            else:
                # å¯¹ä¹‹å‰çš„æ‰€æœ‰å› å­æ­£äº¤åŒ–
                base_factors = [orthogonal_factors[name] for name in factor_order[:i]]
                orthogonal_factors[factor_name] = self.orthogonalize_single(
                    self.factors[factor_name],
                    base_factors
                )

        return orthogonal_factors

    def pca_orthogonalization(self, n_components=None):
        """
        PCAæ­£äº¤åŒ–ï¼šæå–ä¸»æˆåˆ†

        å‚æ•°:
            n_components: int, ä¸»æˆåˆ†æ•°é‡ï¼ˆNoneåˆ™è‡ªåŠ¨é€‰æ‹©ï¼‰

        è¿”å›:
            dict, PCAä¸»æˆåˆ†
        """
        from sklearn.decomposition import PCA
        from sklearn.preprocessing import StandardScaler

        # åˆå¹¶æ‰€æœ‰å› å­
        combined = pd.DataFrame()
        factor_names = list(self.factors.keys())

        for name in factor_names:
            # å±•å¹³
            flat_values = self.factors[name].values.flatten()
            combined[name] = flat_values

        # å»é™¤NaN
        combined_clean = combined.dropna()

        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(combined_clean)

        # PCA
        if n_components is None:
            # é€‰æ‹©ç´¯è®¡è§£é‡Šæ–¹å·® >= 95% çš„æˆåˆ†æ•°
            pca_full = PCA()
            pca_full.fit(X_scaled)
            cum_var_ratio = np.cumsum(pca_full.explained_variance_ratio_)
            n_components = np.argmax(cum_var_ratio >= 0.95) + 1

        pca = PCA(n_components=n_components)
        X_pca = pca.fit_transform(X_scaled)

        # è½¬æ¢ä¸ºå› å­æ ¼å¼ï¼ˆè¿™é‡Œç®€åŒ–ï¼‰
        pca_factors = {}
        for i in range(n_components):
            pca_factors[f'PC{i+1}'] = pd.Series(X_pca[:, i], index=combined_clean.index)

        # æ‰“å°è§£é‡Šæ–¹å·®
        print("\nPCAä¸»æˆåˆ†åˆ†æ:")
        for i in range(n_components):
            print(f"  PC{i+1}: è§£é‡Šæ–¹å·®={pca.explained_variance_ratio_[i]:.2%}")
        print(f"  ç´¯è®¡è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.2%}")

        # æ‰“å°æˆåˆ†è½½è·
        print("\nä¸»æˆåˆ†è½½è·çŸ©é˜µ:")
        loadings = pd.DataFrame(
            pca.components_.T,
            columns=[f'PC{i+1}' for i in range(n_components)],
            index=factor_names
        )
        print(loadings)

        return pca_factors, pca

# ä½¿ç”¨ç¤ºä¾‹
# factor_dict = {
#     'momentum': momentum_factor,
#     'value': value_factor,
#     'quality': quality_factor,
#     'size': size_factor
# }
#
# ortho = FactorOrthogonalization(factor_dict)
#
# # æŸ¥çœ‹å› å­ç›¸å…³æ€§
# corr_matrix = ortho.calculate_factor_correlation()
#
# # Schmidtæ­£äº¤åŒ–ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰
# orthogonal_factors = ortho.schmidt_orthogonalization(
#     factor_order=['value', 'quality', 'momentum', 'size']
# )
#
# # PCAæ­£äº¤åŒ–
# pca_factors, pca_model = ortho.pca_orthogonalization(n_components=3)
```

---

## 18.4 å¤šå› å­æ¨¡å‹æ„å»º

å•ä¸ªå› å­å¾€å¾€ä¸å¤Ÿç¨³å®šï¼Œç»„åˆå¤šä¸ªå› å­å¯ä»¥æé«˜ç­–ç•¥çš„ç¨³å¥æ€§ã€‚

### â— 18.4.1 å› å­åˆæˆæ–¹æ³•

**1. ç­‰æƒåˆæˆ**

```python
def equal_weight_combine(factor_dict):
    """
    ç­‰æƒåˆæˆï¼šæ‰€æœ‰å› å­æƒé‡ç›¸ç­‰

    æœ€ç®€å•çš„æ–¹æ³•ï¼Œé€‚åˆå› å­è´¨é‡ç›¸è¿‘çš„æƒ…å†µ
    """
    # å…ˆæ ‡å‡†åŒ–æ¯ä¸ªå› å­
    normalized_factors = {}

    for name, factor in factor_dict.items():
        # æˆªé¢æ ‡å‡†åŒ–
        normalized = (factor - factor.mean(axis=1, keepdims=True)) / \
                     factor.std(axis=1, keepdims=True)
        normalized_factors[name] = normalized

    # ç­‰æƒå¹³å‡
    combined = sum(normalized_factors.values()) / len(normalized_factors)

    return combined
```

**2. ICåŠ æƒ**

```python
def ic_weight_combine(factor_dict, return_data, ic_window=60):
    """
    ICåŠ æƒï¼šæ ¹æ®å†å²ICåŠ æƒ

    ICé«˜çš„å› å­æƒé‡å¤§
    """
    # è®¡ç®—æ¯ä¸ªå› å­çš„å†å²IC
    factor_ics = {}

    for name, factor in factor_dict.items():
        ics = []

        for i in range(ic_window, len(factor)):
            date = factor.index[i]

            if date not in return_data.index:
                continue

            # è¿‡å»ic_windowå¤©çš„å¹³å‡IC
            ic_values = []
            for j in range(i - ic_window, i):
                hist_date = factor.index[j]
                future_return = return_data.loc[date]

                factor_val = factor.loc[hist_date]

                valid_mask = factor_val.notna() & future_return.notna()
                if valid_mask.sum() < 10:
                    continue

                ic = factor_val[valid_mask].corr(future_return[valid_mask])
                ic_values.append(ic)

            avg_ic = np.mean(ic_values) if len(ic_values) > 0 else 0
            ics.append(avg_ic)

        factor_ics[name] = ics

    # æ ¹æ®ICåŠ æƒ
    # æƒé‡ = IC / sum(IC)
    combined_factor = None

    for i, date in enumerate(factor.index[ic_window:]):
        weights = {}
        total_ic = 0

        for name in factor_dict.keys():
            ic = factor_ics[name][i] if i < len(factor_ics[name]) else 0
            ic = max(ic, 0)  # åªè€ƒè™‘æ­£IC
            weights[name] = ic
            total_ic += ic

        if total_ic == 0:
            continue

        # å½’ä¸€åŒ–æƒé‡
        for name in weights:
            weights[name] /= total_ic

        # åˆæˆå› å­
        combined_value = sum(
            weights[name] * factor_dict[name].loc[date]
            for name in factor_dict.keys()
        )

        if combined_factor is None:
            combined_factor = pd.DataFrame(index=factor.index, columns=factor.columns)

        combined_factor.loc[date] = combined_value

    return combined_factor
```

**3. IC_IRåŠ æƒ**

```python
def ic_ir_weight_combine(factor_dict, return_data, lookback=60):
    """
    IC_IRåŠ æƒï¼šæ ¹æ®ICå’ŒICIRçš„ç»„åˆåŠ æƒ

    è€ƒè™‘äº†ICçš„ç¨³å®šæ€§
    """
    weights = {}

    for name, factor in factor_dict.items():
        # è®¡ç®—ICåºåˆ—
        ics = []

        for date in factor.index:
            if date not in return_data.index:
                continue

            factor_val = factor.loc[date]
            future_return = return_data.loc[date]

            valid_mask = factor_val.notna() & future_return.notna()
            if valid_mask.sum() < 10:
                continue

            ic = factor_val[valid_mask].corr(future_return[valid_mask])
            ics.append(ic)

        # ICå’ŒICIR
        mean_ic = np.mean(ics)
        ic_ir = mean_ic / np.std(ics) if np.std(ics) > 0 else 0

        # æƒé‡ = IC * ICIR (è€ƒè™‘äº†æ”¶ç›Šå’Œç¨³å®šæ€§)
        weights[name] = max(mean_ic * ic_ir, 0)

    # å½’ä¸€åŒ–
    total_weight = sum(weights.values())
    if total_weight > 0:
        weights = {k: v/total_weight for k, v in weights.items()}

    # åˆæˆ
    combined = sum(
        weights[name] * factor
        for name, factor in factor_dict.items()
    )

    print("\nå› å­æƒé‡:")
    for name, weight in weights.items():
        print(f"  {name}: {weight:.2%}")

    return combined
```

**4. æœ€å¤§åŒ–IC_IRï¼ˆä¼˜åŒ–æ–¹æ³•ï¼‰**

```python
from scipy.optimize import minimize

def optimize_factor_weights(factor_dict, return_data, method='max_ic_ir'):
    """
    ä¼˜åŒ–å› å­æƒé‡

    æ–¹æ³•:
        - 'max_ic_ir': æœ€å¤§åŒ–ç»„åˆIC_IR
        - 'max_sharpe': æœ€å¤§åŒ–ç»„åˆå¤æ™®æ¯”ç‡
        - 'min_variance': æœ€å°åŒ–ç»„åˆæ–¹å·®
    """
    # è®¡ç®—æ¯ä¸ªå› å­çš„ICåºåˆ—
    factor_ic_series = {}

    for name, factor in factor_dict.items():
        ics = []

        for date in factor.index:
            if date not in return_data.index:
                continue

            factor_val = factor.loc[date]
            future_return = return_data.loc[date]

            valid_mask = factor_val.notna() & future_return.notna()
            if valid_mask.sum() < 10:
                continue

            ic = factor_val[valid_mask].corr(future_return[valid_mask])
            ics.append(ic)

        factor_ic_series[name] = np.array(ics)

    # æ„å»ºICçŸ©é˜µ
    factor_names = list(factor_dict.keys())
    n_factors = len(factor_names)

    ic_matrix = np.column_stack([factor_ic_series[name] for name in factor_names])

    # ä¼˜åŒ–ç›®æ ‡å‡½æ•°
    def objective(weights):
        # ç»„åˆIC
        combined_ic = ic_matrix @ weights

        if method == 'max_ic_ir':
            # æœ€å¤§åŒ–IC_IR = ICå‡å€¼ / ICæ ‡å‡†å·®
            ic_mean = np.mean(combined_ic)
            ic_std = np.std(combined_ic)
            ic_ir = ic_mean / ic_std if ic_std > 0 else 0
            return -ic_ir  # è´Ÿå·å› ä¸ºminimize

        elif method == 'max_sharpe':
            # æœ€å¤§åŒ–ICå‡å€¼ï¼Œæƒ©ç½šICæ ‡å‡†å·®
            ic_mean = np.mean(combined_ic)
            ic_std = np.std(combined_ic)
            return -(ic_mean - 0.5 * ic_std)

        elif method == 'min_variance':
            # æœ€å°åŒ–ICæ–¹å·®
            ic_var = np.var(combined_ic)
            return ic_var

    # çº¦æŸæ¡ä»¶
    constraints = [
        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1},  # æƒé‡å’Œä¸º1
    ]

    # è¾¹ç•Œï¼ˆæƒé‡åœ¨0-1ä¹‹é—´ï¼‰
    bounds = [(0, 1) for _ in range(n_factors)]

    # åˆå§‹æƒé‡ï¼ˆç­‰æƒï¼‰
    x0 = np.array([1/n_factors] * n_factors)

    # ä¼˜åŒ–
    result = minimize(objective, x0, method='SLSQP',
                     bounds=bounds, constraints=constraints)

    optimal_weights = dict(zip(factor_names, result.x))

    print(f"\nä¼˜åŒ–æ–¹æ³•: {method}")
    print("æœ€ä¼˜æƒé‡:")
    for name, weight in optimal_weights.items():
        print(f"  {name}: {weight:.2%}")

    # åˆæˆå› å­
    combined = sum(
        optimal_weights[name] * factor
        for name, factor in factor_dict.items()
    )

    return combined, optimal_weights

# ä½¿ç”¨ç¤ºä¾‹
# combined_factor, weights = optimize_factor_weights(
#     factor_dict,
#     return_data,
#     method='max_ic_ir'
# )
```

### â— 18.4.2 é£é™©æ¨¡å‹

åœ¨å¤šå› å­æ¨¡å‹ä¸­ï¼Œéœ€è¦è€ƒè™‘é£é™©å› å­çš„æš´éœ²ï¼Œé¿å…è¿‡åº¦é›†ä¸­ã€‚

```python
class RiskModel:
    """é£é™©æ¨¡å‹ï¼šBarraé£æ ¼"""

    def __init__(self, stock_data):
        self.data = stock_data

    def calculate_style_factors(self):
        """
        è®¡ç®—é£æ ¼å› å­æš´éœ²

        ç»å…¸Barraé£æ ¼å› å­:
        - Size (å¸‚å€¼)
        - Value (ä¼°å€¼)
        - Growth (æˆé•¿)
        - Momentum (åŠ¨é‡)
        - Volatility (æ³¢åŠ¨ç‡)
        - Quality (è´¨é‡)
        - Liquidity (æµåŠ¨æ€§)
        """
        exposures = pd.DataFrame(index=self.data.index)

        # Size
        exposures['size'] = np.log(self.data['market_cap'])

        # Value (å¤šä¸ªä¼°å€¼æŒ‡æ ‡çš„å¹³å‡)
        exposures['value'] = (
            self.data['ep'] +  # EP
            self.data['bp'] +  # BP
            self.data['cfp']   # CFP
        ) / 3

        # Growth
        exposures['growth'] = self.data['revenue_growth']

        # Momentum
        exposures['momentum'] = self.data['return_252d']

        # Volatility
        exposures['volatility'] = self.data['volatility_60d']

        # Quality
        exposures['quality'] = self.data['roe']

        # Liquidity
        exposures['liquidity'] = self.data['turnover_rate']

        # æ ‡å‡†åŒ–
        for col in exposures.columns:
            exposures[col] = (exposures[col] - exposures[col].mean()) / exposures[col].std()

        return exposures

    def calculate_industry_factors(self, industry_classification):
        """
        è®¡ç®—è¡Œä¸šå› å­æš´éœ²

        å‚æ•°:
            industry_classification: dict, {è‚¡ç¥¨ä»£ç : è¡Œä¸š}
        """
        # è¡Œä¸šå“‘å˜é‡
        industries = set(industry_classification.values())
        industry_exposure = pd.DataFrame(0, index=self.data.index, columns=list(industries))

        for stock, industry in industry_classification.items():
            if stock in self.data.index:
                industry_exposure.loc[stock, industry] = 1

        return industry_exposure

    def estimate_factor_covariance(self, factor_returns, method='sample'):
        """
        ä¼°è®¡å› å­åæ–¹å·®çŸ©é˜µ

        æ–¹æ³•:
            - 'sample': æ ·æœ¬åæ–¹å·®
            - 'shrinkage': æ”¶ç¼©ä¼°è®¡
            - 'ewma': æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡
        """
        if method == 'sample':
            cov_matrix = factor_returns.cov()

        elif method == 'shrinkage':
            # Ledoit-Wolfæ”¶ç¼©
            from sklearn.covariance import LedoitWolf
            lw = LedoitWolf()
            lw.fit(factor_returns.dropna())
            cov_matrix = pd.DataFrame(
                lw.covariance_,
                index=factor_returns.columns,
                columns=factor_returns.columns
            )

        elif method == 'ewma':
            # æŒ‡æ•°åŠ æƒ
            span = 60
            cov_matrix = factor_returns.ewm(span=span).cov().iloc[-1]

        return cov_matrix

    def calculate_portfolio_risk(self, weights, factor_exposures, factor_cov):
        """
        è®¡ç®—ç»„åˆé£é™©

        ç»„åˆæ–¹å·® = X' * F * Cov(F) * F' * X + X' * Diag(ÏƒÂ²) * X

        å…¶ä¸­:
            X: ç»„åˆæƒé‡
            F: å› å­æš´éœ²çŸ©é˜µ
            Cov(F): å› å­åæ–¹å·®çŸ©é˜µ
            ÏƒÂ²: ç‰¹è´¨é£é™©
        """
        # å› å­é£é™©
        portfolio_factor_exposure = factor_exposures.T @ weights
        factor_risk = portfolio_factor_exposure.T @ factor_cov @ portfolio_factor_exposure

        # ç‰¹è´¨é£é™©ï¼ˆç®€åŒ–ï¼Œå‡è®¾ä¸ºæ€»é£é™©çš„30%ï¼‰
        specific_risk = factor_risk * 0.3

        # æ€»é£é™©
        total_risk = factor_risk + specific_risk

        return {
            'total_risk': np.sqrt(total_risk),
            'factor_risk': np.sqrt(factor_risk),
            'specific_risk': np.sqrt(specific_risk),
            'factor_exposure': portfolio_factor_exposure
        }

# ä½¿ç”¨ç¤ºä¾‹
# risk_model = RiskModel(stock_data)
#
# # è®¡ç®—å› å­æš´éœ²
# style_exposures = risk_model.calculate_style_factors()
#
# # è®¡ç®—ç»„åˆé£é™©
# portfolio_risk = risk_model.calculate_portfolio_risk(
#     weights,
#     style_exposures,
#     factor_cov_matrix
# )
#
# print(f"ç»„åˆæ€»é£é™©: {portfolio_risk['total_risk']:.2%}")
# print(f"å› å­é£é™©: {portfolio_risk['factor_risk']:.2%}")
# print(f"ç‰¹è´¨é£é™©: {portfolio_risk['specific_risk']:.2%}")
```

### â— 18.4.3 å®Œæ•´çš„å¤šå› å­é€‰è‚¡æµç¨‹

```python
class MultiFactorStockSelection:
    """å®Œæ•´çš„å¤šå› å­é€‰è‚¡ç³»ç»Ÿ"""

    def __init__(self, factor_dict, return_data, risk_model):
        self.factors = factor_dict
        self.returns = return_data
        self.risk_model = risk_model

    def run_selection(self, date, n_stocks=50, constraints=None):
        """
        è¿è¡Œé€‰è‚¡

        å‚æ•°:
            date: é€‰è‚¡æ—¥æœŸ
            n_stocks: é€‰è‚¡æ•°é‡
            constraints: dict, çº¦æŸæ¡ä»¶
                - 'max_industry_weight': å•è¡Œä¸šæœ€å¤§æƒé‡
                - 'max_factor_exposure': å› å­æš´éœ²é™åˆ¶
                - 'min_market_cap': æœ€å°å¸‚å€¼

        è¿”å›:
            dict, é€‰ä¸­çš„è‚¡ç¥¨åŠæƒé‡
        """
        # 1. åˆæˆç»¼åˆå› å­å¾—åˆ†
        composite_score = self.combine_factors(date)

        # 2. åº”ç”¨çº¦æŸç­›é€‰
        if constraints:
            composite_score = self.apply_constraints(composite_score, constraints)

        # 3. é€‰æ‹©top N
        selected_stocks = composite_score.nlargest(n_stocks)

        # 4. è®¡ç®—æƒé‡ï¼ˆè¿™é‡Œç”¨ç­‰æƒï¼Œä¹Ÿå¯ä»¥ç”¨ä¼˜åŒ–æ–¹æ³•ï¼‰
        weights = pd.Series(1/n_stocks, index=selected_stocks.index)

        # 5. é£é™©åˆ†æ
        risk_analysis = self.analyze_portfolio_risk(weights)

        return {
            'stocks': selected_stocks.index.tolist(),
            'weights': weights,
            'scores': selected_stocks,
            'risk_analysis': risk_analysis
        }

    def combine_factors(self, date):
        """åˆæˆå› å­"""
        # è¿™é‡Œä½¿ç”¨ICåŠ æƒï¼ˆä¹Ÿå¯ä»¥ç”¨å…¶ä»–æ–¹æ³•ï¼‰
        combined = equal_weight_combine(
            {name: factor.loc[date] for name, factor in self.factors.items()}
        )
        return combined

    def apply_constraints(self, scores, constraints):
        """åº”ç”¨çº¦æŸ"""
        filtered_scores = scores.copy()

        # å¸‚å€¼çº¦æŸ
        if 'min_market_cap' in constraints:
            # éœ€è¦å¸‚å€¼æ•°æ®
            pass

        # å…¶ä»–çº¦æŸ...

        return filtered_scores

    def analyze_portfolio_risk(self, weights):
        """åˆ†æç»„åˆé£é™©"""
        # ä½¿ç”¨é£é™©æ¨¡å‹
        return self.risk_model.calculate_portfolio_risk(weights)

# ä½¿ç”¨ç¤ºä¾‹
# multi_factor = MultiFactorStockSelection(factor_dict, return_data, risk_model)
#
# result = multi_factor.run_selection(
#     date='2024-01-15',
#     n_stocks=50,
#     constraints={'min_market_cap': 50e8}  # 50äº¿
# )
#
# print("é€‰ä¸­è‚¡ç¥¨:")
# print(result['stocks'])
# print("\né£é™©åˆ†æ:")
# print(result['risk_analysis'])
```

---

## æœ¬ç« å°ç»“

æœ¬ç« æ·±å…¥ä»‹ç»äº†å› å­ç ”ç©¶çš„å®Œæ•´ä½“ç³»ï¼Œè¿™æ˜¯æ„å»ºä¸“ä¸šé‡åŒ–ç­–ç•¥çš„æ ¸å¿ƒåŸºç¡€ã€‚

### ğŸ“š æ ¸å¿ƒè¦ç‚¹

1. **å› å­æŠ•èµ„åŸºç¡€**
   - å› å­æ˜¯è§£é‡Šè‚¡ç¥¨æ”¶ç›Šå·®å¼‚çš„å¯é‡åŒ–ç‰¹å¾
   - ç»å…¸åˆ†ç±»ï¼šæŠ€æœ¯ã€åŸºæœ¬é¢ã€å¦ç±»ã€é£æ ¼å› å­
   - ç†è®ºåŸºç¡€ï¼šé£é™©è¡¥å¿ã€è¡Œä¸ºé‡‘èã€æ•°æ®æŒ–æ˜

2. **å› å­æ„å»ºæ–¹æ³•**
   - æŠ€æœ¯å› å­ï¼šåŠ¨é‡ã€æ³¢åŠ¨ç‡ã€é‡ä»·
   - åŸºæœ¬é¢å› å­ï¼šä¼°å€¼ã€æˆé•¿ã€è´¨é‡ã€è´¢åŠ¡å¥åº·
   - å¦ç±»å› å­ï¼šèˆ†æƒ…ã€åˆ†æå¸ˆã€è‚¡ä¸œã€äº‹ä»¶
   - å› å­è¡¨è¾¾å¼ï¼šçµæ´»çš„ç»„åˆå’Œå˜æ¢

3. **å› å­æœ‰æ•ˆæ€§æ£€éªŒ**
   - ICåˆ†æï¼šICå‡å€¼ã€ICIRã€ICèƒœç‡
   - åˆ†ç»„å›æµ‹ï¼šå•è°ƒæ€§ã€å¤šç©ºæ”¶ç›Š
   - å› å­è¡°å‡ï¼šé¢„æµ‹èƒ½åŠ›çš„æŒç»­æ€§
   - å› å­æ­£äº¤åŒ–ï¼šæ¶ˆé™¤å…±çº¿æ€§

4. **å¤šå› å­æ¨¡å‹æ„å»º**
   - å› å­åˆæˆï¼šç­‰æƒã€ICåŠ æƒã€ä¼˜åŒ–æ–¹æ³•
   - é£é™©æ¨¡å‹ï¼šBarraé£æ ¼å› å­
   - å®Œæ•´æµç¨‹ï¼šä»å› å­åˆ°ç»„åˆ

### âš ï¸ é‡ç‚¹æé†’

1. **å› å­å¿…é¡»æœ‰é€»è¾‘æ”¯æ’‘**
   - çº¯æ•°æ®æŒ–æ˜çš„å› å­å®¹æ˜“è¿‡æ‹Ÿåˆ
   - å¥½çš„å› å­åº”è¯¥æœ‰ç»æµå­¦æˆ–è¡Œä¸ºå­¦è§£é‡Š

2. **ICè¯„åˆ¤æ ‡å‡†**
   - ä¼˜ç§€ï¼š|IC| > 0.05, ICIR > 0.5
   - åˆæ ¼ï¼š|IC| > 0.03, ICIR > 0.3
   - ICè¦ç¨³å®šï¼Œä¸èƒ½å¤§èµ·å¤§è½

3. **åˆ†ç»„å›æµ‹çœ‹å•è°ƒæ€§**
   - ç†æƒ³æƒ…å†µï¼šå®Œå…¨å•è°ƒ
   - å¯æ¥å—ï¼šåŸºæœ¬å•è°ƒï¼ˆ70%ä»¥ä¸Šï¼‰
   - ä¸å•è°ƒçš„å› å­éœ€è¦é‡æ–°æ£€è®¨

4. **å¤šå› å­ä¼˜äºå•å› å­**
   - åˆ†æ•£é£é™©
   - æé«˜ç¨³å¥æ€§
   - æ³¨æ„å› å­ç›¸å…³æ€§

### ğŸ¯ å®è·µå»ºè®®

1. **å»ºç«‹å› å­åº“**
   - ç³»ç»ŸåŒ–ç®¡ç†æ‰€æœ‰å› å­
   - å®šæœŸæ›´æ–°ICç»Ÿè®¡
   - è®°å½•å› å­å¤±æ•ˆæƒ…å†µ

2. **é‡è§†æ ·æœ¬å¤–æµ‹è¯•**
   - ä¸è¦åœ¨å…¨æ ·æœ¬ä¸Šè°ƒå‚
   - ç•™å‡ºæ ·æœ¬å¤–æ•°æ®éªŒè¯
   - å…³æ³¨å®ç›˜è¡¨ç°

3. **å› å­ç ”ç©¶æ˜¯æŒç»­è¿‡ç¨‹**
   - å¸‚åœºåœ¨å˜åŒ–ï¼Œå› å­ä¼šå¤±æ•ˆ
   - éœ€è¦ä¸æ–­æŒ–æ˜æ–°å› å­
   - ä¿æŒå­¦ä¹ å’Œåˆ›æ–°

### ğŸ“– ä¸‹ä¸€æ­¥å­¦ä¹ 

ä¸‹ä¸€ç« æˆ‘ä»¬å°†å­¦ä¹  **å›æµ‹æ¡†æ¶æ·±åº¦åº”ç”¨**ï¼Œå­¦ä¹ å¦‚ä½•ä½¿ç”¨backtraderã€vectorbtã€Qlibç­‰ä¸“ä¸šå›æµ‹æ¡†æ¶éªŒè¯æˆ‘ä»¬æ„å»ºçš„å› å­å’Œç­–ç•¥ã€‚

---

**ğŸ’¡ æ€è€ƒé¢˜**ï¼š

1. ä¸ºä»€ä¹ˆåŠ¨é‡å› å­åœ¨Aè‚¡å¸‚åœºæœ‰æ•ˆï¼Ÿå®ƒèƒŒåçš„è¡Œä¸ºé‡‘èå­¦è§£é‡Šæ˜¯ä»€ä¹ˆï¼Ÿ
2. å¦‚æœä¸€ä¸ªå› å­ICå‡å€¼ä¸º0.04ï¼Œä½†ICIRåªæœ‰0.15ï¼Œä½ ä¼šä½¿ç”¨è¿™ä¸ªå› å­å—ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
3. åœ¨å¤šå› å­åˆæˆæ—¶ï¼Œç­‰æƒã€ICåŠ æƒã€ä¼˜åŒ–æ–¹æ³•å„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ

---

*æœ¬ç« å®Œ*
