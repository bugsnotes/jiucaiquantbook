# ç¬¬äºŒåä¸€ç« ï¼šæœºå™¨å­¦ä¹ é‡åŒ–åº”ç”¨

ğŸ¯ **é€‚åˆäººç¾¤**ï¼šæœ‰ç¼–ç¨‹åŸºç¡€çš„ä¸“ä¸šç”¨æˆ· | â±ï¸ **å­¦ä¹ å‘¨æœŸ**ï¼š2-3å‘¨

## æœ¬ç« å¯¼è¯»

æœºå™¨å­¦ä¹ åœ¨é‡åŒ–äº¤æ˜“ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ã€‚æœ¬ç« ä»‹ç»å¦‚ä½•å°†æœºå™¨å­¦ä¹ æŠ€æœ¯åº”ç”¨äºé‡åŒ–ç­–ç•¥å¼€å‘ã€‚

**æœ¬ç« å†…å®¹**ï¼š
- ğŸ¤– 21.1 æœºå™¨å­¦ä¹ åŸºç¡€
- ğŸ“Š 21.2 ç‰¹å¾å·¥ç¨‹
- ğŸ¯ 21.3 ç»å…¸MLæ¨¡å‹åº”ç”¨
- ğŸ”¥ 21.4 æ·±åº¦å­¦ä¹ ç­–ç•¥

---

## 21.1 æœºå™¨å­¦ä¹ åŸºç¡€

### â— 21.1.1 ç›‘ç£å­¦ä¹ åœ¨é‡åŒ–ä¸­çš„åº”ç”¨

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import pandas as pd

# 1. å‡†å¤‡æ•°æ®
# X: ç‰¹å¾ï¼ˆå› å­ï¼‰
# y: æ ‡ç­¾ï¼ˆæœªæ¥æ”¶ç›Šæ–¹å‘ï¼‰

def prepare_ml_data(factor_data, return_data, forward_days=5):
    """
    å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®

    å‚æ•°:
        factor_data: å› å­æ•°æ®
        return_data: æ”¶ç›Šç‡æ•°æ®
        forward_days: æœªæ¥Nå¤©æ”¶ç›Š
    """
    # è®¡ç®—æœªæ¥æ”¶ç›Š
    future_return = return_data.shift(-forward_days)

    # æ ‡ç­¾ï¼š1è¡¨ç¤ºä¸Šæ¶¨ï¼Œ0è¡¨ç¤ºä¸‹è·Œ
    labels = (future_return > 0).astype(int)

    # å±•å¹³æ•°æ®
    X_list = []
    y_list = []

    for date in factor_data.index:
        if date not in labels.index:
            continue

        X = factor_data.loc[date].values
        y = labels.loc[date].values

        # å»é™¤NaN
        mask = ~(pd.isna(X) | pd.isna(y))
        X_list.append(X[mask])
        y_list.append(y[mask])

    X = np.concatenate(X_list)
    y = np.concatenate(y_list)

    return X, y

# 2. è®­ç»ƒæ¨¡å‹
X, y = prepare_ml_data(factor_data, return_data)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = RandomForestClassifier(n_estimators=100, max_depth=10)
model.fit(X_train, y_train)

# 3. è¯„ä¼°
from sklearn.metrics import accuracy_score, precision_score, recall_score

y_pred = model.predict(X_test)
print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.2%}")
print(f"ç²¾ç¡®ç‡: {precision_score(y_test, y_pred):.2%}")
print(f"å¬å›ç‡: {recall_score(y_test, y_pred):.2%}")
```

---

## 21.2 ç‰¹å¾å·¥ç¨‹

### â— 21.2.1 æ—¶é—´åºåˆ—ç‰¹å¾

```python
def create_ts_features(price_data, volume_data):
    """åˆ›å»ºæ—¶é—´åºåˆ—ç‰¹å¾"""
    features = pd.DataFrame(index=price_data.index)

    # ä»·æ ¼ç‰¹å¾
    features['return_1d'] = price_data.pct_change(1)
    features['return_5d'] = price_data.pct_change(5)
    features['return_20d'] = price_data.pct_change(20)

    # æ³¢åŠ¨ç‡ç‰¹å¾
    features['vol_20d'] = price_data.pct_change().rolling(20).std()

    # å‡çº¿ç‰¹å¾
    features['ma5'] = price_data.rolling(5).mean()
    features['ma20'] = price_data.rolling(20).mean()
    features['ma_ratio'] = features['ma5'] / features['ma20']

    # æˆäº¤é‡ç‰¹å¾
    features['volume_ratio'] = volume_data / volume_data.rolling(20).mean()

    return features
```

---

## 21.3 ç»å…¸MLæ¨¡å‹åº”ç”¨

### â— 21.3.1 XGBoostä»·æ ¼é¢„æµ‹

```python
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit

# 1. å‡†å¤‡æ•°æ®
X, y = prepare_regression_data(features, returns)

# 2. æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
tscv = TimeSeriesSplit(n_splits=5)

# 3. è®­ç»ƒXGBoost
params = {
    'objective': 'reg:squarederror',
    'max_depth': 6,
    'learning_rate': 0.1,
    'n_estimators': 100
}

model = xgb.XGBRegressor(**params)

for train_idx, val_idx in tscv.split(X):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    model.fit(X_train, y_train,
             eval_set=[(X_val, y_val)],
             early_stopping_rounds=10,
             verbose=False)

# 4. é¢„æµ‹
y_pred = model.predict(X_val)

# 5. è¯„ä¼°
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_val, y_pred)
r2 = r2_score(y_val, y_pred)
print(f"MSE: {mse:.4f}, R2: {r2:.4f}")
```

---

## 21.4 æ·±åº¦å­¦ä¹ ç­–ç•¥

### â— 21.4.1 LSTMæ—¶é—´åºåˆ—é¢„æµ‹

```python
import torch
import torch.nn as nn

class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super(LSTMModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)

        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# è®­ç»ƒ
model = LSTMModel(input_dim=10, hidden_dim=64, num_layers=2, output_dim=1)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(100):
    outputs = model(X_train)
    loss = criterion(outputs, y_train)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')
```

---

## æœ¬ç« å°ç»“

æœºå™¨å­¦ä¹ ä¸ºé‡åŒ–äº¤æ˜“æä¾›äº†å¼ºå¤§çš„å·¥å…·ï¼š

### ğŸ“š æ ¸å¿ƒè¦ç‚¹

1. **ç‰¹å¾å·¥ç¨‹æœ€å…³é”®**
   - å¥½çš„ç‰¹å¾æ¯”å¤æ‚æ¨¡å‹æ›´é‡è¦
   - éœ€è¦é¢†åŸŸçŸ¥è¯†æŒ‡å¯¼

2. **é˜²æ­¢è¿‡æ‹Ÿåˆ**
   - ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
   - æ ·æœ¬å¤–æµ‹è¯•å¿…ä¸å¯å°‘

3. **æ¨¡å‹é€‰æ‹©**
   - æ ‘æ¨¡å‹é€‚åˆè¡¨æ ¼æ•°æ®
   - æ·±åº¦å­¦ä¹ é€‚åˆåºåˆ—æ•°æ®

---

*æœ¬ç« å®Œ*
